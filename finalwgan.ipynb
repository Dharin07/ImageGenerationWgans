{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZYFTo1EZbCg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import typing\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 6969\n",
        "# seed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPrYyMdXEvZL",
        "outputId": "3bce2f69-1b6e-478d-8dd6-48504a0e46f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  6969\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7be64a980bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "O-3xDJi7En71",
        "outputId": "de7a6897-e51b-495e-b405-1a38aec54946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in /kaggle/input/.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9e7c7584e286>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = dset.ImageFolder(root=dataroot,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            transform=transforms.Compose([\n\u001b[1;32m      3\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /kaggle/input/."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip NM.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjhYdmoqVVue",
        "outputId": "75b02008-c461-4336-9720-d82f68b75d30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  NM.zip\n",
            "   creating: New folder (2)/New folder/\n",
            "  inflating: New folder (2)/New folder/0_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/1_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/10_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/11_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/12_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/13_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/14_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/15_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/16_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/17_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/18_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/19_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/2_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/20_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/3_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/4_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/5_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/6_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/7_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/8_2000.jpg  \n",
            "  inflating: New folder (2)/New folder/9_2000.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "\n",
        "from keras import mixed_precision\n"
      ],
      "metadata": {
        "id": "4gs7PYqahCq7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# celebA dataset path\n",
        "dataset_path = \"/content/New folder (2)\"\n",
        "\n",
        "# Set the input shape and size for the generator and discriminator\n",
        "batch_size = 128\n",
        "img_shape = (64, 64, 3) # The shape of the input image, input to the discriminator\n",
        "noise_dim = 128 # The dimension of the noise vector, input to the generator\n",
        "model_path = '/content/New folder (2)/New folder'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Define your data generator\n",
        "datagen = ImageDataGenerator(\n",
        "    preprocessing_function=lambda x: (x / 127.5) - 1.0,  # Normalize image pixel values to [-1, 1]\n",
        "    horizontal_flip=True  # Data augmentation\n",
        ")\n",
        "\n",
        "# Create a generator that yields batches of images\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory=dataset_path,  # Path to directory containing images\n",
        "    target_size=img_shape[:2],  # Size of images (height, width)\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,  # Do not use labels\n",
        "    shuffle=True,  # Shuffle the data\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh_4BMU8hbl8",
        "outputId": "cdf81544-7327-4479-b435-da13d723c5fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5CMNPkfWxw6",
        "outputId": "eda0d74e-e419-4e14-f580-459edabab637"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-0.9529412  -0.6313726  -0.2862745 ]\n",
            "   [-1.         -0.69411767 -0.3490196 ]\n",
            "   [-1.         -0.7490196  -0.41960782]\n",
            "   ...\n",
            "   [-0.88235295 -0.44313723  0.03529418]\n",
            "   [-0.90588236 -0.46666664 -0.00392157]\n",
            "   [-0.9529412  -0.49019605 -0.03529412]]\n",
            "\n",
            "  [[-0.8980392  -0.5764706  -0.21568626]\n",
            "   [-0.90588236 -0.58431375 -0.2235294 ]\n",
            "   [-0.96862745 -0.64705884 -0.30196077]\n",
            "   ...\n",
            "   [-0.8666667  -0.40392154  0.06666672]\n",
            "   [-0.88235295 -0.41960782  0.03529418]\n",
            "   [-0.8980392  -0.4352941   0.0196079 ]]\n",
            "\n",
            "  [[-0.9137255  -0.5921569  -0.18431371]\n",
            "   [-0.9607843  -0.6313726  -0.23921567]\n",
            "   [-1.         -0.69411767 -0.3098039 ]\n",
            "   ...\n",
            "   [-0.8901961  -0.42745095  0.02745104]\n",
            "   [-0.8980392  -0.41960782  0.02745104]\n",
            "   [-0.8980392  -0.42745095  0.04313731]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.27843136 -0.5529412  -0.3960784 ]\n",
            "   [ 0.3176471   0.05882359  0.19215691]\n",
            "   [ 0.8039216   0.58431375  0.69411767]\n",
            "   ...\n",
            "   [ 0.5137255   0.41176474  0.13725495]\n",
            "   [-0.16862744 -0.23921567 -0.46666664]\n",
            "   [ 0.73333335  0.64705884  0.47450984]]\n",
            "\n",
            "  [[ 0.92156863  0.70980394  0.827451  ]\n",
            "   [ 0.9764706   0.7882353   0.8980392 ]\n",
            "   [ 1.          0.8901961   0.9764706 ]\n",
            "   ...\n",
            "   [ 0.20784318  0.13725495 -0.12156862]\n",
            "   [ 0.6862745   0.6392157   0.43529415]\n",
            "   [ 1.          1.          0.827451  ]]\n",
            "\n",
            "  [[ 1.          0.8352941   0.9607843 ]\n",
            "   [ 1.          0.8745098   0.9764706 ]\n",
            "   [ 1.          0.9137255   0.99215686]\n",
            "   ...\n",
            "   [ 0.81960785  0.7490196   0.52156866]\n",
            "   [ 0.9372549   0.8901961   0.7019608 ]\n",
            "   [ 1.          0.9843137   0.827451  ]]]\n",
            "\n",
            "\n",
            " [[[ 0.94509804  0.9764706   0.9843137 ]\n",
            "   [ 0.94509804  0.9764706   0.9843137 ]\n",
            "   [ 1.          1.          1.        ]\n",
            "   ...\n",
            "   [ 1.          0.8901961   0.77254903]\n",
            "   [ 1.          0.9607843   0.90588236]\n",
            "   [ 1.          0.9607843   0.90588236]]\n",
            "\n",
            "  [[ 0.94509804  0.9764706   0.9843137 ]\n",
            "   [ 0.94509804  0.9764706   0.9843137 ]\n",
            "   [ 1.          1.          1.        ]\n",
            "   ...\n",
            "   [ 1.          0.8901961   0.77254903]\n",
            "   [ 1.          0.9607843   0.90588236]\n",
            "   [ 1.          0.9607843   0.90588236]]\n",
            "\n",
            "  [[ 0.92156863  0.9372549   0.92941177]\n",
            "   [ 0.92156863  0.9372549   0.92941177]\n",
            "   [ 1.          1.          0.9843137 ]\n",
            "   ...\n",
            "   [ 1.          0.9372549   0.7490196 ]\n",
            "   [ 1.          0.94509804  0.81960785]\n",
            "   [ 1.          0.94509804  0.81960785]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.0745098  -0.4588235  -0.8980392 ]\n",
            "   [-0.0745098  -0.4588235  -0.8980392 ]\n",
            "   [-0.17647058 -0.5686275  -0.9843137 ]\n",
            "   ...\n",
            "   [ 0.3176471  -0.11372548 -0.62352943]\n",
            "   [ 0.0196079  -0.40392154 -0.8745098 ]\n",
            "   [ 0.0196079  -0.40392154 -0.8745098 ]]\n",
            "\n",
            "  [[-0.06666666 -0.45098037 -0.8901961 ]\n",
            "   [-0.06666666 -0.45098037 -0.8901961 ]\n",
            "   [-0.19215685 -0.58431375 -1.        ]\n",
            "   ...\n",
            "   [ 0.34901965 -0.06666666 -0.58431375]\n",
            "   [ 0.7882353   0.3803922  -0.09019607]\n",
            "   [ 0.7882353   0.3803922  -0.09019607]]\n",
            "\n",
            "  [[-0.06666666 -0.45098037 -0.8901961 ]\n",
            "   [-0.06666666 -0.45098037 -0.8901961 ]\n",
            "   [-0.19215685 -0.58431375 -1.        ]\n",
            "   ...\n",
            "   [ 0.34901965 -0.06666666 -0.58431375]\n",
            "   [ 0.7882353   0.3803922  -0.09019607]\n",
            "   [ 0.7882353   0.3803922  -0.09019607]]]\n",
            "\n",
            "\n",
            " [[[ 1.          0.92941177  0.84313726]\n",
            "   [ 1.          0.8666667   0.78039217]\n",
            "   [ 0.81960785  0.6627451   0.5921569 ]\n",
            "   ...\n",
            "   [ 0.9843137   0.9843137   0.9843137 ]\n",
            "   [ 0.9843137   0.9764706   0.9607843 ]\n",
            "   [ 1.          1.          0.9843137 ]]\n",
            "\n",
            "  [[ 0.8509804   0.5137255   0.4666667 ]\n",
            "   [ 0.77254903  0.43529415  0.38823533]\n",
            "   [ 0.94509804  0.62352943  0.5921569 ]\n",
            "   ...\n",
            "   [ 1.          0.99215686  1.        ]\n",
            "   [ 0.99215686  0.9843137   0.96862745]\n",
            "   [ 1.          0.99215686  0.9764706 ]]\n",
            "\n",
            "  [[ 0.8980392   0.18431377  0.22352946]\n",
            "   [ 1.          0.32549024  0.35686278]\n",
            "   [ 0.9529412   0.28627455  0.32549024]\n",
            "   ...\n",
            "   [ 1.          0.9843137   1.        ]\n",
            "   [ 1.          0.99215686  1.        ]\n",
            "   [ 1.          1.          0.9843137 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.56078434  0.00392163 -0.01176471]\n",
            "   [ 0.41960788 -0.1372549  -0.15294117]\n",
            "   [ 0.6156863   0.0196079   0.0196079 ]\n",
            "   ...\n",
            "   [ 1.          0.30980396  0.30980396]\n",
            "   [ 0.8509804   0.07450986  0.09803927]\n",
            "   [ 0.7254902  -0.06666666 -0.03529412]]\n",
            "\n",
            "  [[ 0.41960788 -0.05882353 -0.06666666]\n",
            "   [ 0.5294118   0.01176476  0.0196079 ]\n",
            "   [ 0.5921569   0.01176476  0.0196079 ]\n",
            "   ...\n",
            "   [ 1.          0.254902    0.27058828]\n",
            "   [ 0.56078434 -0.23137254 -0.19999999]\n",
            "   [ 0.69411767 -0.11372548 -0.09019607]]\n",
            "\n",
            "  [[ 0.36470592 -0.09019607 -0.09803921]\n",
            "   [ 0.6156863   0.13725495  0.12941182]\n",
            "   [ 0.64705884  0.082353    0.10588241]\n",
            "   ...\n",
            "   [ 0.8117647   0.03529418  0.05882359]\n",
            "   [ 0.4039216  -0.38823527 -0.35686272]\n",
            "   [ 0.56078434 -0.24705881 -0.2235294 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.27843142 -0.05098039 -0.03529412]\n",
            "   [ 0.27843142 -0.05098039 -0.03529412]\n",
            "   [ 0.30196083 -0.01176471 -0.00392157]\n",
            "   ...\n",
            "   [ 0.27843142 -0.04313725 -0.01176471]\n",
            "   [ 0.52156866  0.19215691  0.36470592]\n",
            "   [ 0.52156866  0.19215691  0.36470592]]\n",
            "\n",
            "  [[ 0.27843142 -0.05098039 -0.03529412]\n",
            "   [ 0.27843142 -0.05098039 -0.03529412]\n",
            "   [ 0.30196083 -0.01176471 -0.00392157]\n",
            "   ...\n",
            "   [ 0.27843142 -0.04313725 -0.01176471]\n",
            "   [ 0.52156866  0.19215691  0.36470592]\n",
            "   [ 0.52156866  0.19215691  0.36470592]]\n",
            "\n",
            "  [[ 0.20784318 -0.12156862 -0.10588235]\n",
            "   [ 0.20784318 -0.12156862 -0.10588235]\n",
            "   [ 0.69411767  0.3803922   0.38823533]\n",
            "   ...\n",
            "   [ 0.28627455 -0.02745098 -0.02745098]\n",
            "   [ 0.3411765   0.03529418  0.18431377]\n",
            "   [ 0.3411765   0.03529418  0.18431377]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.7019608   0.48235297  0.45882356]\n",
            "   [ 0.7019608   0.48235297  0.45882356]\n",
            "   [ 0.41176474  0.17647064  0.16078436]\n",
            "   ...\n",
            "   [ 0.427451    0.17647064  0.18431377]\n",
            "   [-0.2235294  -0.47450978 -0.46666664]\n",
            "   [-0.2235294  -0.47450978 -0.46666664]]\n",
            "\n",
            "  [[ 0.5921569   0.34901965  0.32549024]\n",
            "   [ 0.5921569   0.34901965  0.32549024]\n",
            "   [ 0.33333337  0.06666672  0.05098045]\n",
            "   ...\n",
            "   [ 0.09803927 -0.15294117 -0.14509803]\n",
            "   [-0.21568626 -0.46666664 -0.4588235 ]\n",
            "   [-0.21568626 -0.46666664 -0.4588235 ]]\n",
            "\n",
            "  [[ 0.5921569   0.34901965  0.32549024]\n",
            "   [ 0.5921569   0.34901965  0.32549024]\n",
            "   [ 0.33333337  0.06666672  0.05098045]\n",
            "   ...\n",
            "   [ 0.09803927 -0.15294117 -0.14509803]\n",
            "   [-0.21568626 -0.46666664 -0.4588235 ]\n",
            "   [-0.21568626 -0.46666664 -0.4588235 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.9764706   0.99215686  0.9843137 ]\n",
            "   [ 0.9529412   0.9843137   0.99215686]\n",
            "   [ 0.9764706   1.          1.        ]\n",
            "   ...\n",
            "   [ 0.92156863  0.9137255   0.8980392 ]\n",
            "   [ 1.          0.99215686  1.        ]\n",
            "   [ 0.94509804  0.9607843   0.9529412 ]]\n",
            "\n",
            "  [[ 0.9843137   1.          0.99215686]\n",
            "   [ 0.99215686  1.          1.        ]\n",
            "   [ 0.99215686  1.          1.        ]\n",
            "   ...\n",
            "   [ 0.43529415  0.4039216   0.3803922 ]\n",
            "   [ 0.99215686  0.9607843   0.9372549 ]\n",
            "   [ 0.92941177  0.92156863  0.90588236]]\n",
            "\n",
            "  [[ 0.99215686  1.          1.        ]\n",
            "   [ 0.9607843   0.96862745  0.9843137 ]\n",
            "   [ 0.99215686  1.          1.        ]\n",
            "   ...\n",
            "   [-0.29411763 -0.3333333  -0.36470586]\n",
            "   [-0.00392157 -0.04313725 -0.06666666]\n",
            "   [ 1.          0.9843137   0.9764706 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.9764706   0.99215686  0.96862745]\n",
            "   [ 0.96862745  1.          0.99215686]\n",
            "   [ 0.94509804  0.9764706   0.9843137 ]\n",
            "   ...\n",
            "   [ 0.9607843   0.99215686  0.9843137 ]\n",
            "   [ 0.96862745  1.          0.99215686]\n",
            "   [ 0.96862745  1.          1.        ]]\n",
            "\n",
            "  [[ 0.99215686  0.99215686  0.99215686]\n",
            "   [ 0.9843137   1.          0.99215686]\n",
            "   [ 0.99215686  1.          1.        ]\n",
            "   ...\n",
            "   [ 0.9843137   1.          0.9764706 ]\n",
            "   [ 0.96862745  1.          0.99215686]\n",
            "   [ 0.9607843   0.99215686  0.9843137 ]]\n",
            "\n",
            "  [[ 1.          0.9764706   1.        ]\n",
            "   [ 0.99215686  0.9843137   0.96862745]\n",
            "   [ 0.99215686  1.          0.96862745]\n",
            "   ...\n",
            "   [ 0.9843137   1.          0.9764706 ]\n",
            "   [ 0.99215686  1.          1.        ]\n",
            "   [ 0.9764706   0.9764706   0.9607843 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.6784314   0.654902    0.6156863 ]\n",
            "   [ 0.67058825  0.64705884  0.5921569 ]\n",
            "   [ 0.8352941   0.78039217  0.73333335]\n",
            "   ...\n",
            "   [ 1.          0.7882353   0.7490196 ]\n",
            "   [ 0.99215686  0.77254903  0.7411765 ]\n",
            "   [ 0.9764706   0.75686276  0.73333335]]\n",
            "\n",
            "  [[ 0.8509804   0.8117647   0.7647059 ]\n",
            "   [ 0.9372549   0.8980392   0.8509804 ]\n",
            "   [ 1.          0.9843137   0.92156863]\n",
            "   ...\n",
            "   [ 0.99215686  0.73333335  0.6627451 ]\n",
            "   [ 0.99215686  0.75686276  0.69411767]\n",
            "   [ 1.          0.7882353   0.75686276]]\n",
            "\n",
            "  [[ 0.92156863  0.8509804   0.79607844]\n",
            "   [ 0.9843137   0.9137255   0.84313726]\n",
            "   [ 0.99215686  0.9137255   0.84313726]\n",
            "   ...\n",
            "   [ 0.9843137   0.7019608   0.60784316]\n",
            "   [ 0.99215686  0.7254902   0.64705884]\n",
            "   [ 1.          0.77254903  0.7176471 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.09019607 -0.16862744  0.5058824 ]\n",
            "   [-0.12156862 -0.18431371  0.48235297]\n",
            "   [-0.26274508 -0.3333333   0.36470592]\n",
            "   ...\n",
            "   [-0.12941176 -0.23137254  0.47450984]\n",
            "   [-0.27058822 -0.372549    0.33333337]\n",
            "   [-0.5294118  -0.6156863   0.12156868]]\n",
            "\n",
            "  [[-0.05098039 -0.14509803  0.5294118 ]\n",
            "   [-0.14509803 -0.2235294   0.45098042]\n",
            "   [-0.30196077 -0.38039213  0.3176471 ]\n",
            "   ...\n",
            "   [-0.14509803 -0.24705881  0.4431373 ]\n",
            "   [-0.29411763 -0.3960784   0.30980396]\n",
            "   [-0.5921569  -0.6784314   0.05882359]]\n",
            "\n",
            "  [[-0.04313725 -0.15294117  0.5294118 ]\n",
            "   [-0.14509803 -0.23921567  0.43529415]\n",
            "   [-0.32549018 -0.40392154  0.2941177 ]\n",
            "   ...\n",
            "   [-0.21568626 -0.29411763  0.38823533]\n",
            "   [-0.3490196  -0.42745095  0.27058828]\n",
            "   [-0.64705884 -0.73333335  0.00392163]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "latent_dim=128\n",
        "# Define the generator model\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgRjg8ThhlpZ",
        "outputId": "190ee5bb-ef77-45d4-a6de-699ab9fc389e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 16, 16, 128)       262272    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 32, 32, 256)       524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 64, 64, 512)       2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 64, 64, 3)         38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3979651 (15.18 MB)\n",
            "Trainable params: 3979651 (15.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "\n",
        "discriminator= keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"linear\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP-LvExGhzAW",
        "outputId": "9aa26d14-5d67-42ba-fd40-fd660d8fd1d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404801 (1.54 MB)\n",
            "Trainable params: 404801 (1.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(\n",
        "        self,\n",
        "        real_samples: tf.Tensor,\n",
        "        fake_samples: tf.Tensor,\n",
        "        discriminator: tf.keras.models.Model\n",
        "    ) -> tf.Tensor:\n",
        "    \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "    Gradient penalty is calculated on an interpolated data\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(real_samples)[0]\n",
        "\n",
        "    # Generate random values for epsilon\n",
        "    epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n",
        "\n",
        "    # 1. Interpolate between real and fake samples\n",
        "    interpolated_samples = epsilon * real_samples + ((1 - epsilon) * fake_samples)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_samples)\n",
        "        # 2. Get the discriminator's output for the interpolated image\n",
        "        logits = discriminator(interpolated_samples, training=True)\n",
        "\n",
        "    # 3. Calculate the gradients w.r.t to the interpolated image\n",
        "    gradients = tape.gradient(logits, interpolated_samples)\n",
        "\n",
        "    # 4. Calculate the L2 norm of the gradients.\n",
        "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "\n",
        "    # 5. Calculate gradient penalty\n",
        "    gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n",
        "\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "M_SJl2k1h3Cl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WGAN_GP(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "            self,\n",
        "            discriminator: tf.keras.models.Model,\n",
        "            generator: tf.keras.models.Model,\n",
        "            noise_dim: int,\n",
        "            discriminator_extra_steps: int=5,\n",
        "            gp_weight: typing.Union[float, int]=10.0\n",
        "        ) -> None:\n",
        "        super(WGAN_GP, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.noise_dim = noise_dim\n",
        "        self.discriminator_extra_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(\n",
        "            self,\n",
        "            discriminator_opt: tf.keras.optimizers.Optimizer,\n",
        "            generator_opt: tf.keras.optimizers.Optimizer,\n",
        "            discriminator_loss: typing.Callable,\n",
        "            generator_loss: typing.Callable,\n",
        "            **kwargs\n",
        "        ) -> None:\n",
        "        super(WGAN_GP, self).compile(**kwargs)\n",
        "        self.discriminator_opt = discriminator_opt\n",
        "        self.generator_opt = generator_opt\n",
        "        self.discriminator_loss = discriminator_loss\n",
        "        self.generator_loss = generator_loss\n",
        "\n",
        "    def add_instance_noise(self, x: tf.Tensor, stddev: float=0.1) -> tf.Tensor:\n",
        "        \"\"\" Adds instance noise to the input tensor.\"\"\"\n",
        "        noise = tf.random.normal(tf.shape(x), mean=0.0, stddev=stddev, dtype=x.dtype)\n",
        "        return x + noise\n",
        "\n",
        "    def gradient_penalty(\n",
        "            self,\n",
        "            real_samples: tf.Tensor,\n",
        "            fake_samples: tf.Tensor,\n",
        "            discriminator: tf.keras.models.Model\n",
        "        ) -> tf.Tensor:\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        Gradient penalty is calculated on an interpolated data\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(real_samples)[0]\n",
        "\n",
        "        # Generate random values for epsilon\n",
        "        epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n",
        "\n",
        "        # 1. Interpolate between real and fake samples\n",
        "        interpolated_samples = epsilon * real_samples + ((1 - epsilon) * fake_samples)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(interpolated_samples)\n",
        "            # 2. Get the discriminator's output for the interpolated image\n",
        "            logits = discriminator(interpolated_samples, training=True)\n",
        "\n",
        "        # 3. Calculate the gradients w.r.t to the interpolated image\n",
        "        gradients = tape.gradient(logits, interpolated_samples)\n",
        "\n",
        "        # 4. Calculate the L2 norm of the gradients.\n",
        "        gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "\n",
        "        # 5. Calculate gradient penalty\n",
        "        gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n",
        "\n",
        "        return gradient_penalty\n",
        "\n",
        "    def train_step(self, real_samples: tf.Tensor) -> typing.Dict[str, float]:\n",
        "        batch_size = tf.shape(real_samples)[0]\n",
        "        noise = tf.random.normal([batch_size, self.noise_dim])\n",
        "        gps = []\n",
        "\n",
        "        # Step 1. Train the discriminator with both real and fake samples\n",
        "        # Train the discriminator more often than the generator\n",
        "        for _ in range(self.discriminator_extra_steps):\n",
        "\n",
        "            # Step 1. Train the discriminator with both real images and fake images\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_samples = self.generator(noise, training=True)\n",
        "                pred_real = self.discriminator(real_samples, training=True)\n",
        "                pred_fake = self.discriminator(fake_samples, training=True)\n",
        "\n",
        "                # Add instance noise to real and fake samples\n",
        "                real_samples = self.add_instance_noise(real_samples)\n",
        "                fake_samples = self.add_instance_noise(fake_samples)\n",
        "\n",
        "                # Calculate the WGAN-GP gradient penalty\n",
        "                gp = self.gradient_penalty(real_samples, fake_samples, self.discriminator)\n",
        "                gps.append(gp)\n",
        "\n",
        "                # Add gradient penalty to the original discriminator loss\n",
        "                disc_loss = self.discriminator_loss(pred_real, pred_fake) + gp * self.gp_weight\n",
        "\n",
        "            # Compute discriminator gradients\n",
        "            grads = tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "            # Update discriminator weights\n",
        "            self.discriminator_opt.apply_gradients(zip(grads, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Step 2. Train the generator\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_samples = self.generator(noise, training=True)\n",
        "            pred_fake = self.discriminator(fake_samples, training=True)\n",
        "            gen_loss = self.generator_loss(pred_fake)\n",
        "\n",
        "        # Compute generator gradients\n",
        "        grads = tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "\n",
        "        # Update generator wieghts\n",
        "        self.generator_opt.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
        "\n",
        "        # Update the metrics.\n",
        "        # Metrics are configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(real_samples, fake_samples)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"d_loss\": disc_loss, \"g_loss\": gen_loss, \"gp\": tf.reduce_mean(gps)})\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "-F9aRo5bh7Jc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\" Callback for generating and saving images during training.\"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            noise_dim: int,\n",
        "            output_path: str,\n",
        "            examples_to_generate: int=16,\n",
        "            grid_size: tuple=(4, 4),\n",
        "            spacing: int=5,\n",
        "            gif_size: tuple=(416, 416),\n",
        "            duration: float=0.1,\n",
        "            save_model: bool=True\n",
        "        ) -> None:\n",
        "        super(ResultsCallback, self).__init__()\n",
        "        self.seed = tf.random.normal([examples_to_generate, noise_dim])\n",
        "        self.results = []\n",
        "        self.output_path = output_path\n",
        "        self.results_path = output_path + '/results'\n",
        "        self.grid_size = grid_size\n",
        "        self.spacing = spacing\n",
        "        self.gif_size = gif_size\n",
        "        self.duration = duration\n",
        "        self.save_model = save_model\n",
        "\n",
        "        os.makedirs(self.results_path, exist_ok=True)\n",
        "\n",
        "    def save_plt(self, epoch: int, results: np.ndarray):\n",
        "        # construct an image from generated images with spacing between them using numpy\n",
        "        w, h , c = results[0].shape\n",
        "        # construct grind with self.grid_size\n",
        "        grid = np.zeros((self.grid_size[0] * w + (self.grid_size[0] - 1) * self.spacing, self.grid_size[1] * h + (self.grid_size[1] - 1) * self.spacing, c), dtype=np.uint8)\n",
        "        for i in range(self.grid_size[0]):\n",
        "            for j in range(self.grid_size[1]):\n",
        "                grid[i * (w + self.spacing):i * (w + self.spacing) + w, j * (h + self.spacing):j * (h + self.spacing) + h] = results[i * self.grid_size[1] + j]\n",
        "\n",
        "        grid = cv2.cvtColor(grid, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # save the image\n",
        "        cv2.imwrite(f'{self.results_path}/img_{epoch}.png', grid)\n",
        "\n",
        "        # save image to memory resized to gif size\n",
        "        self.results.append(cv2.resize(grid, self.gif_size, interpolation=cv2.INTER_AREA))\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: dict=None):\n",
        "        # Define your custom code here that should be executed at the end of each epoch\n",
        "        predictions = self.model.generator(self.seed, training=False)\n",
        "        predictions_uint8 = (predictions * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "        self.save_plt(epoch, predictions_uint8)\n",
        "\n",
        "        if self.save_model:\n",
        "            # save keras model to disk\n",
        "            models_path = os.path.join(self.output_path, \"model\")\n",
        "            os.makedirs(models_path, exist_ok=True)\n",
        "            self.model.discriminator.save(models_path + \"/discriminator.h5\")\n",
        "            self.model.generator.save(models_path + \"/generator.h5\")\n",
        "\n",
        "    def on_train_end(self, logs: dict=None):\n",
        "        # save the results as a gif with imageio\n",
        "\n",
        "        # Create a list of imageio image objects from the OpenCV images\n",
        "        # image is in BGR format, convert to RGB format when loading\n",
        "        imageio_images = [imageio.core.util.Image(image[...,::-1]) for image in self.results]\n",
        "\n",
        "        # Write the imageio images to a GIF file\n",
        "        imageio.mimsave(self.results_path + \"/output.gif\", imageio_images, duration=self.duration)"
      ],
      "metadata": {
        "id": "0BbOLehSiEBp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.utils.array_to_img(generated_images[i])\n",
        "            img.save(\"/content/output.png\")"
      ],
      "metadata": {
        "id": "2RwGEH1WYzYR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LRSheduler(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Learning rate scheduler for WGAN-GP\"\"\"\n",
        "    def __init__(self, decay_epochs: int, tb_callback=None, min_lr: float=0.00001):\n",
        "        super(LRSheduler, self).__init__()\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.min_lr = min_lr\n",
        "        self.tb_callback = tb_callback\n",
        "        self.compiled = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if not self.compiled:\n",
        "            self.generator_lr = self.model.generator_opt.lr.numpy()\n",
        "            self.discriminator_lr = self.model.discriminator_opt.lr.numpy()\n",
        "            self.compiled = True\n",
        "\n",
        "        if epoch < self.decay_epochs:\n",
        "            new_g_lr = max(self.generator_lr * (1 - (epoch / self.decay_epochs)), self.min_lr)\n",
        "            self.model.generator_opt.lr.assign(new_g_lr)\n",
        "            new_d_lr = max(self.discriminator_lr * (1 - (epoch / self.decay_epochs)), self.min_lr)\n",
        "            self.model.discriminator_opt.lr.assign(new_d_lr)\n",
        "            print(f\"Learning rate generator: {new_g_lr}, discriminator: {new_d_lr}\")\n",
        "\n",
        "            # Log the learning rate on TensorBoard\n",
        "            if self.tb_callback is not None:\n",
        "                writer = self.tb_callback._writers.get('train')  # get the writer from the TensorBoard callback\n",
        "                with writer.as_default():\n",
        "                    tf.summary.scalar('generator_lr', data=new_g_lr, step=epoch)\n",
        "                    tf.summary.scalar('discriminator_lr', data=new_d_lr, step=epoch)\n",
        "                    writer.flush()"
      ],
      "metadata": {
        "id": "4cXBZ2TDiwk8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wasserstein loss for the discriminator\n",
        "def discriminator_w_loss(pred_real, pred_fake):\n",
        "    real_loss = tf.reduce_mean(pred_real)\n",
        "    fake_loss = tf.reduce_mean(pred_fake)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "# Wasserstein loss for the generator\n",
        "def generator_w_loss(pred_fake):\n",
        "    return -tf.reduce_mean(pred_fake)"
      ],
      "metadata": {
        "id": "pHDIodX1i0Nz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5, beta_2=0.9)\n"
      ],
      "metadata": {
        "id": "yUB79ikKl_Du"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = ResultsCallback(noise_dim=noise_dim, output_path=model_path, duration=0.04)\n",
        "tb_callback = TensorBoard(model_path + '/logs')\n",
        "lr_scheduler = LRSheduler(decay_epochs=500, tb_callback=tb_callback)\n",
        "\n",
        "gan = WGAN_GP(discriminator=discriminator, generator=generator, noise_dim=noise_dim, discriminator_extra_steps=5)\n",
        "gan.compile(discriminator_optimizer, generator_optimizer, discriminator_w_loss, generator_w_loss, run_eagerly=False,metrics=[\"accuracy\"])\n",
        "\n",
        "history=gan.fit(train_generator, epochs=1000, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hC6OuZ-i3SH",
        "outputId": "43cccf0d-6b90-4a21-d55f-b4412af8d971"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 6s 6s/step - accuracy: 0.5630 - d_loss: -95.4059 - g_loss: -12.5911 - gp: 4.3391\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5497 - d_loss: -91.8594 - g_loss: -3.8041 - gp: 4.4931\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 458ms/step - accuracy: 0.5526 - d_loss: -94.6962 - g_loss: -5.5907 - gp: 4.3626\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 455ms/step - accuracy: 0.5656 - d_loss: -97.9303 - g_loss: -5.8415 - gp: 4.4760\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.5479 - d_loss: -94.2705 - g_loss: 17.6360 - gp: 4.8378\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 446ms/step - accuracy: 0.5465 - d_loss: -92.3698 - g_loss: 19.5342 - gp: 4.4383\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 447ms/step - accuracy: 0.5509 - d_loss: -98.9331 - g_loss: 0.7068 - gp: 4.7338\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 443ms/step - accuracy: 0.5794 - d_loss: -98.3330 - g_loss: -19.0355 - gp: 4.9783\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 445ms/step - accuracy: 0.5556 - d_loss: -91.2389 - g_loss: -2.9229 - gp: 4.8925\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 443ms/step - accuracy: 0.5500 - d_loss: -92.5786 - g_loss: -1.6679 - gp: 4.3731\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 443ms/step - accuracy: 0.5310 - d_loss: -93.7097 - g_loss: 7.7657 - gp: 4.6698\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 439ms/step - accuracy: 0.5324 - d_loss: -103.5407 - g_loss: -1.3270 - gp: 4.7549\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 444ms/step - accuracy: 0.4803 - d_loss: -103.7744 - g_loss: 8.2874 - gp: 5.1385\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.4519 - d_loss: -101.1445 - g_loss: 21.2758 - gp: 5.2380\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.4231 - d_loss: -104.3281 - g_loss: 8.5095 - gp: 5.2565\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 445ms/step - accuracy: 0.4202 - d_loss: -102.9767 - g_loss: 17.4068 - gp: 5.5150\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 445ms/step - accuracy: 0.4279 - d_loss: -100.2613 - g_loss: -4.4912 - gp: 5.2547\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 448ms/step - accuracy: 0.4415 - d_loss: -97.2451 - g_loss: -5.3885 - gp: 5.0383\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.4469 - d_loss: -99.2426 - g_loss: -6.5215 - gp: 4.9653\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 444ms/step - accuracy: 0.4634 - d_loss: -95.7065 - g_loss: 0.1627 - gp: 5.1110\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 446ms/step - accuracy: 0.4845 - d_loss: -99.3189 - g_loss: -2.8907 - gp: 4.7648\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 449ms/step - accuracy: 0.5054 - d_loss: -105.0882 - g_loss: -2.2081 - gp: 5.0354\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5413 - d_loss: -98.9346 - g_loss: 4.9587 - gp: 5.2100\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 435ms/step - accuracy: 0.5531 - d_loss: -107.7771 - g_loss: 11.8722 - gp: 4.9548\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.5734 - d_loss: -97.0864 - g_loss: 5.4986 - gp: 5.4243\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.5873 - d_loss: -101.9895 - g_loss: -11.2862 - gp: 4.7603\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 468ms/step - accuracy: 0.5996 - d_loss: -95.7485 - g_loss: -4.2598 - gp: 4.8197\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6137 - d_loss: -101.5850 - g_loss: -2.1543 - gp: 5.2232\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 448ms/step - accuracy: 0.6092 - d_loss: -99.4962 - g_loss: -10.4210 - gp: 4.8491\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.5901 - d_loss: -97.5836 - g_loss: 0.9636 - gp: 5.0214\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 449ms/step - accuracy: 0.5787 - d_loss: -95.2430 - g_loss: 27.6708 - gp: 5.1601\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5956 - d_loss: -98.7626 - g_loss: 2.1600 - gp: 4.6238\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 449ms/step - accuracy: 0.5905 - d_loss: -98.2736 - g_loss: -0.6178 - gp: 4.9779\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 444ms/step - accuracy: 0.5959 - d_loss: -94.6804 - g_loss: -3.9642 - gp: 5.0817\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.5907 - d_loss: -95.4261 - g_loss: -5.8717 - gp: 4.6123\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 448ms/step - accuracy: 0.6138 - d_loss: -93.5962 - g_loss: -19.6835 - gp: 4.8284\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 446ms/step - accuracy: 0.5936 - d_loss: -94.2872 - g_loss: -4.8245 - gp: 4.7283\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 448ms/step - accuracy: 0.5883 - d_loss: -89.7189 - g_loss: 6.6091 - gp: 4.7088\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5957 - d_loss: -99.4444 - g_loss: -11.8429 - gp: 4.6503\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 449ms/step - accuracy: 0.5977 - d_loss: -93.8718 - g_loss: 7.4321 - gp: 5.1221\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 447ms/step - accuracy: 0.5877 - d_loss: -96.0103 - g_loss: 5.2150 - gp: 4.5528\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.5765 - d_loss: -96.3521 - g_loss: 3.9008 - gp: 4.8169\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.6020 - d_loss: -95.3224 - g_loss: -7.7860 - gp: 4.7439\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 448ms/step - accuracy: 0.5783 - d_loss: -91.6137 - g_loss: 7.0311 - gp: 4.9816\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.5902 - d_loss: -87.6633 - g_loss: 6.4133 - gp: 4.5901\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.5902 - d_loss: -93.0008 - g_loss: -3.9505 - gp: 4.4952\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.5962 - d_loss: -98.0418 - g_loss: 0.4119 - gp: 4.9394\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.5795 - d_loss: -97.4992 - g_loss: 10.9604 - gp: 4.9435\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.6003 - d_loss: -94.1959 - g_loss: 4.1914 - gp: 4.8831\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6069 - d_loss: -93.0155 - g_loss: -2.1986 - gp: 4.5975\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.6301 - d_loss: -98.6750 - g_loss: -22.7978 - gp: 5.0969\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6073 - d_loss: -92.7995 - g_loss: 11.7848 - gp: 4.8075\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6105 - d_loss: -92.0426 - g_loss: 6.0788 - gp: 4.4955\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.6214 - d_loss: -95.8363 - g_loss: -7.9629 - gp: 4.9494\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.6117 - d_loss: -91.0424 - g_loss: 7.2397 - gp: 4.6849\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.6245 - d_loss: -91.8279 - g_loss: 9.0599 - gp: 4.7021\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.6327 - d_loss: -96.2041 - g_loss: 1.4556 - gp: 4.5106\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.6407 - d_loss: -92.4908 - g_loss: -3.1199 - gp: 4.6712\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.6423 - d_loss: -91.5041 - g_loss: -5.2068 - gp: 4.5890\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 455ms/step - accuracy: 0.6411 - d_loss: -91.8062 - g_loss: -12.5409 - gp: 4.5568\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.6438 - d_loss: -97.5007 - g_loss: -4.8532 - gp: 4.5250\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 434ms/step - accuracy: 0.6400 - d_loss: -94.8253 - g_loss: 11.4180 - gp: 4.8300\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.6370 - d_loss: -91.4524 - g_loss: 20.9144 - gp: 4.8783\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 453ms/step - accuracy: 0.6293 - d_loss: -91.2359 - g_loss: -3.4750 - gp: 4.5255\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 455ms/step - accuracy: 0.6240 - d_loss: -91.3673 - g_loss: -7.9977 - gp: 4.7557\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.5944 - d_loss: -93.3555 - g_loss: -10.1359 - gp: 4.3996\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5720 - d_loss: -96.2642 - g_loss: 2.5566 - gp: 4.6315\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.5397 - d_loss: -88.7600 - g_loss: 20.4287 - gp: 4.9083\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.4847 - d_loss: -91.8814 - g_loss: 10.7871 - gp: 4.5446\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 457ms/step - accuracy: 0.4212 - d_loss: -92.8512 - g_loss: 7.1523 - gp: 4.6908\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 453ms/step - accuracy: 0.3680 - d_loss: -95.6494 - g_loss: -0.8726 - gp: 4.6840\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 451ms/step - accuracy: 0.3818 - d_loss: -97.3163 - g_loss: 26.6507 - gp: 5.0628\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 458ms/step - accuracy: 0.3666 - d_loss: -101.9079 - g_loss: 4.3626 - gp: 4.9024\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.3257 - d_loss: -92.9370 - g_loss: -15.7011 - gp: 5.1964\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.3587 - d_loss: -100.5456 - g_loss: -3.1540 - gp: 4.6558\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.3455 - d_loss: -95.5804 - g_loss: -18.3188 - gp: 4.9180\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 455ms/step - accuracy: 0.3666 - d_loss: -91.8508 - g_loss: -20.3566 - gp: 4.8252\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 457ms/step - accuracy: 0.4296 - d_loss: -98.6879 - g_loss: -12.6323 - gp: 4.5595\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.4390 - d_loss: -98.1151 - g_loss: -19.4587 - gp: 4.6040\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.4974 - d_loss: -89.8586 - g_loss: -22.2977 - gp: 4.9454\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.5314 - d_loss: -92.8691 - g_loss: -20.7025 - gp: 4.8677\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 454ms/step - accuracy: 0.5669 - d_loss: -93.4920 - g_loss: 14.3816 - gp: 4.8524\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.5690 - d_loss: -92.5727 - g_loss: -16.6017 - gp: 4.4285\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 452ms/step - accuracy: 0.5898 - d_loss: -94.6738 - g_loss: -24.0591 - gp: 4.6390\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6161 - d_loss: -95.9096 - g_loss: -29.1080 - gp: 5.1116\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.6370 - d_loss: -89.8718 - g_loss: -6.0950 - gp: 4.8728\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.6386 - d_loss: -98.0049 - g_loss: 11.7402 - gp: 4.6881\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.6358 - d_loss: -91.4884 - g_loss: -13.5728 - gp: 4.5912\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 457ms/step - accuracy: 0.6381 - d_loss: -96.8115 - g_loss: -23.6162 - gp: 4.5363\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 456ms/step - accuracy: 0.6375 - d_loss: -94.2630 - g_loss: -11.7392 - gp: 4.8304\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6426 - d_loss: -88.6353 - g_loss: -18.4733 - gp: 4.8167\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6453 - d_loss: -91.9962 - g_loss: -10.3635 - gp: 4.6776\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6479 - d_loss: -92.7850 - g_loss: 12.2630 - gp: 4.6094\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6468 - d_loss: -93.3359 - g_loss: -0.7108 - gp: 4.5820\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6469 - d_loss: -90.4985 - g_loss: -10.1210 - gp: 4.6200\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6450 - d_loss: -96.8358 - g_loss: 28.3976 - gp: 4.7206\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.6502 - d_loss: -89.4541 - g_loss: -16.5433 - gp: 4.2730\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6475 - d_loss: -97.2092 - g_loss: -3.0823 - gp: 4.5147\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6491 - d_loss: -94.7550 - g_loss: 12.9869 - gp: 4.8218\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6499 - d_loss: -90.6506 - g_loss: -6.9134 - gp: 4.5555\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 465ms/step - accuracy: 0.6510 - d_loss: -90.5756 - g_loss: 11.0283 - gp: 4.6131\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 458ms/step - accuracy: 0.6490 - d_loss: -88.4127 - g_loss: -6.6594 - gp: 4.6173\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.6497 - d_loss: -91.2738 - g_loss: -14.8393 - gp: 4.4355\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6480 - d_loss: -88.0069 - g_loss: 7.0988 - gp: 4.6091\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6484 - d_loss: -89.0166 - g_loss: -4.1522 - gp: 4.5763\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6521 - d_loss: -95.1786 - g_loss: -3.0917 - gp: 4.4262\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 458ms/step - accuracy: 0.6485 - d_loss: -87.0031 - g_loss: -7.9714 - gp: 4.7980\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6467 - d_loss: -91.5282 - g_loss: -16.6040 - gp: 4.4601\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6482 - d_loss: -85.4236 - g_loss: -11.6841 - gp: 4.6002\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.6488 - d_loss: -92.0942 - g_loss: 1.4412 - gp: 4.2932\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6468 - d_loss: -89.9347 - g_loss: -8.7749 - gp: 4.3351\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.6460 - d_loss: -89.8854 - g_loss: -7.7316 - gp: 4.5964\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6456 - d_loss: -87.8921 - g_loss: 2.2738 - gp: 4.4161\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6447 - d_loss: -89.4232 - g_loss: -9.3841 - gp: 4.4158\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.6379 - d_loss: -88.7512 - g_loss: -10.3457 - gp: 4.5666\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6403 - d_loss: -91.8661 - g_loss: -6.5666 - gp: 4.3586\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 468ms/step - accuracy: 0.6396 - d_loss: -81.6351 - g_loss: 23.7594 - gp: 4.7633\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6320 - d_loss: -87.0925 - g_loss: 12.1771 - gp: 4.2454\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 468ms/step - accuracy: 0.6227 - d_loss: -82.5511 - g_loss: -0.8507 - gp: 4.4197\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.6112 - d_loss: -85.0914 - g_loss: -2.1383 - gp: 4.3674\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5872 - d_loss: -85.7426 - g_loss: -13.4328 - gp: 4.2694\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5705 - d_loss: -81.1767 - g_loss: -9.6635 - gp: 4.3306\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 468ms/step - accuracy: 0.5479 - d_loss: -93.4175 - g_loss: -26.5473 - gp: 4.4030\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.5303 - d_loss: -92.5687 - g_loss: -26.1124 - gp: 4.6429\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.5411 - d_loss: -86.4568 - g_loss: -11.4397 - gp: 4.7756\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.5346 - d_loss: -92.8314 - g_loss: -23.6123 - gp: 4.4666\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 455ms/step - accuracy: 0.5486 - d_loss: -85.1208 - g_loss: -17.1348 - gp: 4.3153\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.5645 - d_loss: -87.1057 - g_loss: 0.0603 - gp: 4.4032\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.5684 - d_loss: -90.8044 - g_loss: -14.4728 - gp: 4.1508\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 459ms/step - accuracy: 0.5672 - d_loss: -87.4932 - g_loss: -20.1770 - gp: 4.6376\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 457ms/step - accuracy: 0.5798 - d_loss: -89.4269 - g_loss: -13.3116 - gp: 4.3370\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.5845 - d_loss: -88.0308 - g_loss: -17.8681 - gp: 4.5190\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.5976 - d_loss: -95.7427 - g_loss: -1.8287 - gp: 4.3231\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6088 - d_loss: -85.7960 - g_loss: -1.7492 - gp: 4.5655\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 460ms/step - accuracy: 0.6209 - d_loss: -87.8184 - g_loss: -11.1938 - gp: 4.3991\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6302 - d_loss: -86.7198 - g_loss: -8.3996 - gp: 4.4020\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.6309 - d_loss: -89.3899 - g_loss: -13.3248 - gp: 4.4567\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.6357 - d_loss: -93.7641 - g_loss: -11.1303 - gp: 4.2947\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6390 - d_loss: -87.7698 - g_loss: -9.6528 - gp: 4.6939\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 465ms/step - accuracy: 0.6376 - d_loss: -87.5069 - g_loss: -10.5549 - gp: 4.3331\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6338 - d_loss: -87.4737 - g_loss: -8.5005 - gp: 4.4438\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 461ms/step - accuracy: 0.6380 - d_loss: -87.5277 - g_loss: -13.3714 - gp: 4.2150\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6392 - d_loss: -80.5868 - g_loss: -16.4297 - gp: 4.4330\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6383 - d_loss: -81.4938 - g_loss: -4.3688 - gp: 4.3173\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6384 - d_loss: -87.4512 - g_loss: -25.6418 - gp: 4.0748\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6365 - d_loss: -82.9032 - g_loss: -15.3250 - gp: 4.3187\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.6407 - d_loss: -82.2998 - g_loss: -15.0878 - gp: 4.2552\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6384 - d_loss: -85.7029 - g_loss: -19.1769 - gp: 4.1011\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6356 - d_loss: -86.3629 - g_loss: -4.6092 - gp: 4.2671\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 465ms/step - accuracy: 0.6358 - d_loss: -80.9679 - g_loss: 0.3205 - gp: 4.3274\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 462ms/step - accuracy: 0.6338 - d_loss: -82.9291 - g_loss: -12.4824 - gp: 3.9097\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 463ms/step - accuracy: 0.6302 - d_loss: -80.1073 - g_loss: -9.0912 - gp: 4.0454\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.6215 - d_loss: -92.2716 - g_loss: -26.8851 - gp: 3.9966\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.6120 - d_loss: -81.2601 - g_loss: -15.8867 - gp: 4.3973\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6092 - d_loss: -83.9552 - g_loss: -9.4083 - gp: 3.9133\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.5898 - d_loss: -79.2771 - g_loss: -9.1386 - gp: 4.1083\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.5452 - d_loss: -83.6380 - g_loss: -2.0381 - gp: 4.0607\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.5308 - d_loss: -81.7287 - g_loss: 1.3681 - gp: 4.2100\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.5271 - d_loss: -84.2299 - g_loss: -4.1612 - gp: 4.0484\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.5512 - d_loss: -85.7283 - g_loss: 4.5336 - gp: 4.3561\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.5431 - d_loss: -83.4455 - g_loss: -28.1963 - gp: 4.1216\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 466ms/step - accuracy: 0.5717 - d_loss: -83.0193 - g_loss: -17.4152 - gp: 4.0409\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.5909 - d_loss: -83.1388 - g_loss: -12.8877 - gp: 4.1611\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.6058 - d_loss: -84.7001 - g_loss: -7.2253 - gp: 4.0213\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6039 - d_loss: -89.5054 - g_loss: -9.7544 - gp: 3.9885\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5997 - d_loss: -79.2730 - g_loss: -22.6051 - gp: 4.2119\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6083 - d_loss: -79.6980 - g_loss: -17.4130 - gp: 3.9458\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6221 - d_loss: -84.2236 - g_loss: -7.1747 - gp: 3.9661\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6291 - d_loss: -86.5546 - g_loss: -4.6107 - gp: 4.1440\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.6238 - d_loss: -85.3923 - g_loss: -11.0069 - gp: 3.9950\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.6185 - d_loss: -79.4283 - g_loss: -6.3713 - gp: 4.0844\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6184 - d_loss: -79.8762 - g_loss: -17.0874 - gp: 4.0254\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6180 - d_loss: -83.8719 - g_loss: -17.1748 - gp: 3.9639\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6220 - d_loss: -84.6462 - g_loss: 6.8699 - gp: 4.1110\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.6258 - d_loss: -81.8489 - g_loss: -2.8103 - gp: 4.0473\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 468ms/step - accuracy: 0.6161 - d_loss: -81.6620 - g_loss: -8.6628 - gp: 4.1232\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 464ms/step - accuracy: 0.6067 - d_loss: -83.4272 - g_loss: -5.2923 - gp: 3.8193\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.5969 - d_loss: -83.1560 - g_loss: -10.6053 - gp: 4.1111\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6021 - d_loss: -79.1354 - g_loss: -2.7833 - gp: 4.0289\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.5995 - d_loss: -84.7196 - g_loss: -3.3157 - gp: 3.8122\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.6110 - d_loss: -80.8066 - g_loss: 4.2433 - gp: 4.1028\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6106 - d_loss: -85.1886 - g_loss: -1.3771 - gp: 3.9252\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.6129 - d_loss: -83.5627 - g_loss: -7.6465 - gp: 3.9937\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6122 - d_loss: -83.0897 - g_loss: 1.6030 - gp: 3.8952\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.6168 - d_loss: -82.9578 - g_loss: 5.2952 - gp: 3.9494\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6140 - d_loss: -81.8145 - g_loss: 1.5603 - gp: 3.9159\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6108 - d_loss: -85.4472 - g_loss: 9.7642 - gp: 4.2262\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6153 - d_loss: -86.3511 - g_loss: -7.2944 - gp: 3.9138\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6075 - d_loss: -81.8792 - g_loss: -10.5501 - gp: 4.2285\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6107 - d_loss: -84.1181 - g_loss: -4.8915 - gp: 4.0997\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6125 - d_loss: -85.9123 - g_loss: -3.7887 - gp: 3.9583\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6083 - d_loss: -81.8380 - g_loss: 1.9445 - gp: 3.9582\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6025 - d_loss: -84.9689 - g_loss: -3.7840 - gp: 3.7460\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 467ms/step - accuracy: 0.6126 - d_loss: -82.4343 - g_loss: 4.3700 - gp: 4.1241\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6156 - d_loss: -88.3086 - g_loss: 6.5512 - gp: 4.0488\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.6211 - d_loss: -80.6722 - g_loss: -1.6499 - gp: 4.0821\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6250 - d_loss: -81.9722 - g_loss: -4.8757 - gp: 3.8516\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6250 - d_loss: -83.8677 - g_loss: -1.1646 - gp: 3.8725\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.6284 - d_loss: -82.3105 - g_loss: 6.3612 - gp: 3.9817\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.6298 - d_loss: -86.3376 - g_loss: 2.2816 - gp: 3.9847\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6346 - d_loss: -84.2890 - g_loss: 7.6486 - gp: 4.2775\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6413 - d_loss: -80.4801 - g_loss: 25.4927 - gp: 4.5365\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 469ms/step - accuracy: 0.6407 - d_loss: -85.8797 - g_loss: 18.1382 - gp: 3.9284\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.6361 - d_loss: -90.3154 - g_loss: 24.0695 - gp: 4.3804\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 1s 523ms/step - accuracy: 0.6361 - d_loss: -81.0221 - g_loss: 11.9771 - gp: 4.2667\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6351 - d_loss: -82.8689 - g_loss: 6.7776 - gp: 3.9994\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6299 - d_loss: -83.4293 - g_loss: 6.5300 - gp: 4.1291\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6290 - d_loss: -78.9929 - g_loss: 15.3641 - gp: 4.1989\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 470ms/step - accuracy: 0.6320 - d_loss: -80.5010 - g_loss: 15.9903 - gp: 3.8546\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6379 - d_loss: -78.7193 - g_loss: 17.9281 - gp: 4.1277\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.6382 - d_loss: -82.3057 - g_loss: 13.7404 - gp: 3.8761\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6382 - d_loss: -79.1919 - g_loss: 22.9741 - gp: 3.9478\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6428 - d_loss: -85.2003 - g_loss: 35.7963 - gp: 4.4148\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6420 - d_loss: -84.9889 - g_loss: 12.1225 - gp: 4.2024\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6429 - d_loss: -79.3565 - g_loss: 15.0617 - gp: 4.2441\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6427 - d_loss: -86.0466 - g_loss: 11.6700 - gp: 4.0315\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6384 - d_loss: -82.1150 - g_loss: 19.5407 - gp: 4.1649\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6357 - d_loss: -83.5033 - g_loss: 19.1986 - gp: 4.0587\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6329 - d_loss: -80.9883 - g_loss: 15.1385 - gp: 4.1119\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.6355 - d_loss: -78.2704 - g_loss: 18.7180 - gp: 4.0125\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6313 - d_loss: -85.4299 - g_loss: 18.8060 - gp: 3.8795\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6330 - d_loss: -81.8012 - g_loss: 20.8047 - gp: 4.3082\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6262 - d_loss: -83.6897 - g_loss: 17.7474 - gp: 3.8665\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 450ms/step - accuracy: 0.6212 - d_loss: -81.5247 - g_loss: 11.5691 - gp: 3.9275\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.6150 - d_loss: -84.4094 - g_loss: 15.7658 - gp: 3.8822\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.6093 - d_loss: -79.4307 - g_loss: 12.0844 - gp: 4.1091\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6051 - d_loss: -81.8736 - g_loss: 20.3173 - gp: 4.0096\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 471ms/step - accuracy: 0.6118 - d_loss: -77.3197 - g_loss: 23.7826 - gp: 4.1754\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 473ms/step - accuracy: 0.6034 - d_loss: -82.2174 - g_loss: 12.9396 - gp: 3.6766\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 472ms/step - accuracy: 0.6007 - d_loss: -84.3362 - g_loss: 6.6280 - gp: 4.0027\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.6104 - d_loss: -80.3088 - g_loss: 17.0875 - gp: 4.1370\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.6027 - d_loss: -84.3065 - g_loss: 12.1848 - gp: 3.9195\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6066 - d_loss: -80.2141 - g_loss: 8.6617 - gp: 3.9888\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.6130 - d_loss: -80.4037 - g_loss: 17.5716 - gp: 3.8560\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6117 - d_loss: -83.4794 - g_loss: 14.2831 - gp: 3.9817\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.6188 - d_loss: -78.6635 - g_loss: 22.2465 - gp: 4.1342\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6247 - d_loss: -80.9505 - g_loss: 31.5015 - gp: 4.1292\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6303 - d_loss: -79.2836 - g_loss: 21.9004 - gp: 4.0678\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6241 - d_loss: -81.3728 - g_loss: 23.1212 - gp: 3.9165\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6342 - d_loss: -82.2698 - g_loss: 32.5702 - gp: 4.1111\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6352 - d_loss: -78.6434 - g_loss: 17.7237 - gp: 3.9567\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6388 - d_loss: -83.3567 - g_loss: 24.1498 - gp: 4.1845\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6390 - d_loss: -78.4266 - g_loss: 14.2696 - gp: 4.0233\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6351 - d_loss: -86.3924 - g_loss: 16.2850 - gp: 3.9877\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6293 - d_loss: -79.9192 - g_loss: 20.0697 - gp: 4.3365\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6268 - d_loss: -80.2997 - g_loss: 31.6070 - gp: 4.2477\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6265 - d_loss: -87.0122 - g_loss: 25.4539 - gp: 4.0381\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6243 - d_loss: -85.5730 - g_loss: 24.0237 - gp: 4.0853\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6245 - d_loss: -77.3363 - g_loss: 25.7037 - gp: 4.2368\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6245 - d_loss: -83.1408 - g_loss: 23.9925 - gp: 3.8608\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6231 - d_loss: -85.6639 - g_loss: 13.8156 - gp: 3.9148\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6217 - d_loss: -77.8634 - g_loss: 27.9956 - gp: 4.2472\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6147 - d_loss: -82.4765 - g_loss: 21.5048 - gp: 3.8683\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6118 - d_loss: -77.0512 - g_loss: 30.8540 - gp: 4.2316\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6155 - d_loss: -78.9126 - g_loss: 24.0801 - gp: 3.8677\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6127 - d_loss: -83.0211 - g_loss: 28.0920 - gp: 3.9600\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6100 - d_loss: -78.3993 - g_loss: 25.2457 - gp: 3.9731\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 1s 511ms/step - accuracy: 0.6055 - d_loss: -83.9558 - g_loss: 18.9862 - gp: 3.7820\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5981 - d_loss: -87.8651 - g_loss: 19.2888 - gp: 3.8613\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6042 - d_loss: -78.2437 - g_loss: 30.7953 - gp: 4.1133\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5949 - d_loss: -82.0815 - g_loss: 16.9853 - gp: 3.7483\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5940 - d_loss: -79.8024 - g_loss: 30.4206 - gp: 4.1147\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.5870 - d_loss: -82.0098 - g_loss: 23.1099 - gp: 3.8912\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5858 - d_loss: -76.8202 - g_loss: 28.6938 - gp: 4.1562\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5745 - d_loss: -77.8144 - g_loss: 27.5215 - gp: 3.9179\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5838 - d_loss: -85.1300 - g_loss: 25.8246 - gp: 3.7577\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5883 - d_loss: -81.6542 - g_loss: 40.5138 - gp: 4.2662\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5946 - d_loss: -81.4042 - g_loss: 21.9299 - gp: 3.7998\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5866 - d_loss: -79.1911 - g_loss: 23.9766 - gp: 3.8941\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6009 - d_loss: -85.3734 - g_loss: 24.6882 - gp: 3.9069\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.6014 - d_loss: -77.9819 - g_loss: 25.0918 - gp: 4.2848\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6106 - d_loss: -82.3898 - g_loss: 38.7648 - gp: 4.0771\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6129 - d_loss: -88.3047 - g_loss: 40.9482 - gp: 3.9794\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6163 - d_loss: -78.0805 - g_loss: 31.7227 - gp: 4.1672\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6223 - d_loss: -81.3930 - g_loss: 30.1541 - gp: 4.1617\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6223 - d_loss: -89.3286 - g_loss: 32.4034 - gp: 3.9993\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6230 - d_loss: -79.0065 - g_loss: 28.7318 - gp: 4.2890\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 1s 501ms/step - accuracy: 0.6220 - d_loss: -78.4340 - g_loss: 44.0654 - gp: 4.4066\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 1s 508ms/step - accuracy: 0.6254 - d_loss: -84.4935 - g_loss: 23.6783 - gp: 3.8346\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 500ms/step - accuracy: 0.6191 - d_loss: -79.2447 - g_loss: 31.4081 - gp: 4.1644\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6217 - d_loss: -80.0131 - g_loss: 38.4245 - gp: 4.2031\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6201 - d_loss: -79.3897 - g_loss: 30.2331 - gp: 4.1652\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6207 - d_loss: -83.1020 - g_loss: 39.9966 - gp: 4.0504\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6222 - d_loss: -76.9625 - g_loss: 31.4477 - gp: 3.9962\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6177 - d_loss: -80.1833 - g_loss: 29.6190 - gp: 3.8418\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6205 - d_loss: -83.0464 - g_loss: 30.4871 - gp: 4.1223\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6186 - d_loss: -80.0017 - g_loss: 39.3069 - gp: 4.2604\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6220 - d_loss: -80.3132 - g_loss: 41.1269 - gp: 4.0629\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6209 - d_loss: -80.1352 - g_loss: 28.7476 - gp: 3.8935\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6246 - d_loss: -83.1263 - g_loss: 27.8266 - gp: 3.9521\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6167 - d_loss: -84.7434 - g_loss: 24.2163 - gp: 4.0387\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6187 - d_loss: -77.1415 - g_loss: 28.7581 - gp: 4.2635\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6108 - d_loss: -78.9145 - g_loss: 32.2248 - gp: 4.0845\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6082 - d_loss: -84.3695 - g_loss: 36.1484 - gp: 4.0714\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6100 - d_loss: -81.9566 - g_loss: 42.2639 - gp: 4.1607\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6093 - d_loss: -80.9420 - g_loss: 24.8524 - gp: 3.8925\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6074 - d_loss: -85.4033 - g_loss: 28.6242 - gp: 3.9992\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6089 - d_loss: -80.2060 - g_loss: 26.0759 - gp: 4.1239\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.6105 - d_loss: -86.0320 - g_loss: 29.1167 - gp: 4.0095\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.6094 - d_loss: -87.6654 - g_loss: 28.8418 - gp: 4.0982\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6094 - d_loss: -83.9516 - g_loss: 31.6248 - gp: 4.3279\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 1s 505ms/step - accuracy: 0.6064 - d_loss: -89.3857 - g_loss: 34.7168 - gp: 4.0945\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6047 - d_loss: -86.5580 - g_loss: 38.3806 - gp: 4.2750\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6018 - d_loss: -82.7721 - g_loss: 17.1197 - gp: 4.1206\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6014 - d_loss: -80.0634 - g_loss: 18.1285 - gp: 4.3560\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6065 - d_loss: -81.6276 - g_loss: 22.1136 - gp: 4.1151\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5987 - d_loss: -83.8032 - g_loss: 28.0669 - gp: 3.9286\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6045 - d_loss: -79.6745 - g_loss: 37.8912 - gp: 4.4840\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6116 - d_loss: -84.2931 - g_loss: 22.2969 - gp: 3.8085\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6092 - d_loss: -79.1075 - g_loss: 25.0073 - gp: 4.1459\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6085 - d_loss: -82.4125 - g_loss: 30.4881 - gp: 3.9930\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6089 - d_loss: -78.8761 - g_loss: 27.6596 - gp: 4.0794\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.6059 - d_loss: -79.6389 - g_loss: 22.2529 - gp: 4.0116\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6003 - d_loss: -83.8174 - g_loss: 27.2403 - gp: 3.9645\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6044 - d_loss: -80.4206 - g_loss: 21.9111 - gp: 4.0594\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5966 - d_loss: -80.1264 - g_loss: 40.7290 - gp: 4.1671\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6061 - d_loss: -79.9575 - g_loss: 23.3021 - gp: 4.1421\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6011 - d_loss: -81.1750 - g_loss: 35.3209 - gp: 4.2229\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6043 - d_loss: -78.5669 - g_loss: 25.5353 - gp: 4.2453\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6058 - d_loss: -81.6307 - g_loss: 29.6244 - gp: 4.1068\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6012 - d_loss: -83.2818 - g_loss: 23.5928 - gp: 3.9665\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6040 - d_loss: -73.6574 - g_loss: 25.5466 - gp: 4.1986\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5970 - d_loss: -84.0723 - g_loss: 38.4852 - gp: 3.9459\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5984 - d_loss: -85.7999 - g_loss: 7.8528 - gp: 3.8973\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 1s 508ms/step - accuracy: 0.5972 - d_loss: -80.6134 - g_loss: 25.7561 - gp: 4.2667\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5969 - d_loss: -78.6279 - g_loss: 17.0877 - gp: 4.1360\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5989 - d_loss: -83.6612 - g_loss: 17.7957 - gp: 4.1074\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5956 - d_loss: -81.2933 - g_loss: 28.9452 - gp: 4.1342\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5903 - d_loss: -79.1549 - g_loss: 34.0204 - gp: 4.1246\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5942 - d_loss: -81.3217 - g_loss: 29.6301 - gp: 3.8137\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5941 - d_loss: -84.6647 - g_loss: 13.6541 - gp: 3.8880\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5900 - d_loss: -79.2831 - g_loss: 17.8249 - gp: 4.0799\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5974 - d_loss: -80.9162 - g_loss: 22.3059 - gp: 4.0202\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5971 - d_loss: -83.2480 - g_loss: 30.0262 - gp: 4.0158\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5973 - d_loss: -83.3787 - g_loss: 20.6417 - gp: 4.0824\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5967 - d_loss: -79.7228 - g_loss: 20.4320 - gp: 4.0438\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6002 - d_loss: -82.8029 - g_loss: 21.6448 - gp: 4.1693\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6008 - d_loss: -92.0636 - g_loss: 31.9932 - gp: 4.0271\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6027 - d_loss: -79.7140 - g_loss: 27.7341 - gp: 4.6004\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5980 - d_loss: -91.5720 - g_loss: 43.1005 - gp: 4.1985\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6006 - d_loss: -83.7610 - g_loss: 6.6715 - gp: 4.1021\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5930 - d_loss: -80.1060 - g_loss: 17.9525 - gp: 4.2385\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5873 - d_loss: -79.8769 - g_loss: 15.1068 - gp: 4.1625\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5854 - d_loss: -75.2528 - g_loss: 24.6670 - gp: 4.1540\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5870 - d_loss: -85.7238 - g_loss: 25.4475 - gp: 3.9558\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5952 - d_loss: -85.6762 - g_loss: 23.1168 - gp: 4.3292\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5947 - d_loss: -82.5197 - g_loss: 24.8082 - gp: 4.2379\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5773 - d_loss: -81.8390 - g_loss: 39.9254 - gp: 4.1798\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5747 - d_loss: -83.1028 - g_loss: 23.4237 - gp: 4.1521\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5684 - d_loss: -84.8716 - g_loss: 13.7326 - gp: 3.8889\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5602 - d_loss: -83.4949 - g_loss: 10.4173 - gp: 3.9851\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5611 - d_loss: -78.4022 - g_loss: 20.9472 - gp: 4.0141\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5572 - d_loss: -83.0426 - g_loss: 18.2370 - gp: 3.8842\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5639 - d_loss: -84.7424 - g_loss: 34.5250 - gp: 4.1101\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5674 - d_loss: -79.1708 - g_loss: 23.9118 - gp: 4.0759\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5776 - d_loss: -77.3809 - g_loss: 20.9395 - gp: 4.2849\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5875 - d_loss: -82.5714 - g_loss: 31.7619 - gp: 4.0995\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5845 - d_loss: -78.1105 - g_loss: 16.5131 - gp: 4.2514\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5933 - d_loss: -91.4076 - g_loss: 24.0680 - gp: 3.8792\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5955 - d_loss: -80.1903 - g_loss: 28.6004 - gp: 4.3917\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5963 - d_loss: -80.0490 - g_loss: 21.6851 - gp: 3.9233\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5894 - d_loss: -79.1372 - g_loss: 22.7554 - gp: 4.0483\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6009 - d_loss: -86.8623 - g_loss: 31.5747 - gp: 3.9262\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5989 - d_loss: -78.8423 - g_loss: 42.7148 - gp: 4.2980\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6027 - d_loss: -83.9185 - g_loss: 30.0696 - gp: 4.1460\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6122 - d_loss: -78.9141 - g_loss: 41.4561 - gp: 4.5484\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6123 - d_loss: -84.1124 - g_loss: 38.1421 - gp: 4.2797\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6069 - d_loss: -78.9927 - g_loss: 27.8702 - gp: 3.8858\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.6055 - d_loss: -85.7945 - g_loss: 29.6063 - gp: 3.9824\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5949 - d_loss: -84.8722 - g_loss: 33.1618 - gp: 4.2099\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6039 - d_loss: -79.6315 - g_loss: 33.1216 - gp: 4.3026\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6068 - d_loss: -79.7915 - g_loss: 36.8553 - gp: 4.1558\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6126 - d_loss: -84.5425 - g_loss: 34.1677 - gp: 4.0345\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6178 - d_loss: -83.8639 - g_loss: 19.1671 - gp: 4.0393\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6105 - d_loss: -76.3357 - g_loss: 32.8841 - gp: 4.2328\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6126 - d_loss: -83.4366 - g_loss: 36.7763 - gp: 3.8951\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6078 - d_loss: -77.1877 - g_loss: 32.0285 - gp: 3.9834\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6048 - d_loss: -83.7830 - g_loss: 31.3153 - gp: 3.8533\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6093 - d_loss: -79.1131 - g_loss: 32.9135 - gp: 4.2398\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6031 - d_loss: -82.5334 - g_loss: 33.5512 - gp: 3.9474\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6058 - d_loss: -79.2259 - g_loss: 36.2741 - gp: 4.4905\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6071 - d_loss: -85.8903 - g_loss: 36.7330 - gp: 4.0490\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6063 - d_loss: -77.2716 - g_loss: 35.6885 - gp: 4.1145\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6087 - d_loss: -85.4750 - g_loss: 23.7468 - gp: 3.7117\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6026 - d_loss: -77.7775 - g_loss: 27.0147 - gp: 4.1813\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5966 - d_loss: -82.9068 - g_loss: 29.9402 - gp: 3.9730\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5947 - d_loss: -85.2787 - g_loss: 45.2620 - gp: 4.2141\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.6024 - d_loss: -74.5851 - g_loss: 25.2184 - gp: 4.2156\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5788 - d_loss: -80.7331 - g_loss: 16.3725 - gp: 3.7852\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5800 - d_loss: -82.1031 - g_loss: 25.6843 - gp: 4.2528\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5553 - d_loss: -72.1267 - g_loss: 20.4305 - gp: 4.5354\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5532 - d_loss: -84.4286 - g_loss: 30.8733 - gp: 4.0078\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5552 - d_loss: -81.3975 - g_loss: 42.6791 - gp: 4.5438\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5324 - d_loss: -80.8012 - g_loss: 33.1909 - gp: 4.1471\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5087 - d_loss: -77.3453 - g_loss: 15.6121 - gp: 3.9496\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5122 - d_loss: -75.6813 - g_loss: 19.4020 - gp: 3.9596\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5000 - d_loss: -77.4943 - g_loss: 16.3901 - gp: 3.7355\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5065 - d_loss: -82.7261 - g_loss: 23.1828 - gp: 3.7443\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.5335 - d_loss: -72.8653 - g_loss: 23.1762 - gp: 4.0373\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5279 - d_loss: -75.8612 - g_loss: 18.9390 - gp: 3.6452\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5268 - d_loss: -83.8327 - g_loss: 20.7256 - gp: 3.7903\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5103 - d_loss: -82.0500 - g_loss: 23.2535 - gp: 4.1774\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5078 - d_loss: -69.3468 - g_loss: 32.0540 - gp: 4.2394\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5351 - d_loss: -82.2333 - g_loss: 29.1130 - gp: 3.6717\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5643 - d_loss: -73.3256 - g_loss: 42.2742 - gp: 4.1662\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5907 - d_loss: -75.0368 - g_loss: 24.0177 - gp: 3.5709\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5858 - d_loss: -80.4650 - g_loss: 36.7621 - gp: 3.6741\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5980 - d_loss: -73.5619 - g_loss: 31.8253 - gp: 3.8608\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6132 - d_loss: -77.0891 - g_loss: 29.5872 - gp: 3.6426\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5893 - d_loss: -78.6773 - g_loss: 39.8994 - gp: 3.7693\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6033 - d_loss: -81.1336 - g_loss: 40.3311 - gp: 3.6711\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5923 - d_loss: -73.4098 - g_loss: 35.1177 - gp: 4.0903\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5581 - d_loss: -85.4825 - g_loss: 31.4441 - gp: 3.5045\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5492 - d_loss: -71.9481 - g_loss: 29.9965 - gp: 4.1415\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.5305 - d_loss: -78.5047 - g_loss: 32.1195 - gp: 3.6706\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5547 - d_loss: -76.6878 - g_loss: 45.5616 - gp: 3.8436\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5873 - d_loss: -72.2931 - g_loss: 38.0068 - gp: 3.6882\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6028 - d_loss: -79.3537 - g_loss: 52.5495 - gp: 3.7114\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5714 - d_loss: -81.5578 - g_loss: 20.1628 - gp: 3.5934\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5472 - d_loss: -78.7215 - g_loss: 28.1967 - gp: 3.8152\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5685 - d_loss: -77.8518 - g_loss: 35.8287 - gp: 3.8670\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5523 - d_loss: -79.3539 - g_loss: 42.9416 - gp: 3.7590\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5781 - d_loss: -73.9598 - g_loss: 34.4923 - gp: 3.9414\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5705 - d_loss: -78.9122 - g_loss: 33.7735 - gp: 3.4574\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5722 - d_loss: -76.0674 - g_loss: 28.4275 - gp: 3.7331\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5547 - d_loss: -77.4385 - g_loss: 43.8369 - gp: 3.5558\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5725 - d_loss: -70.9136 - g_loss: 45.8018 - gp: 3.9799\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5657 - d_loss: -79.3909 - g_loss: 38.0755 - gp: 3.5856\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.5774 - d_loss: -77.8843 - g_loss: 57.8360 - gp: 4.0232\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5587 - d_loss: -78.0296 - g_loss: 33.7165 - gp: 3.5976\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5282 - d_loss: -74.7672 - g_loss: 40.7602 - gp: 3.7548\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5724 - d_loss: -77.6903 - g_loss: 40.0861 - gp: 3.7473\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 1s 500ms/step - accuracy: 0.5727 - d_loss: -81.5665 - g_loss: 41.0885 - gp: 3.5593\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5714 - d_loss: -74.9623 - g_loss: 40.8357 - gp: 3.8563\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.5851 - d_loss: -75.0359 - g_loss: 46.5372 - gp: 3.5946\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5787 - d_loss: -77.4243 - g_loss: 54.5649 - gp: 3.3982\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.5700 - d_loss: -71.9666 - g_loss: 47.8307 - gp: 3.5212\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 474ms/step - accuracy: 0.5316 - d_loss: -82.6453 - g_loss: 49.1508 - gp: 3.3537\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5266 - d_loss: -72.7160 - g_loss: 46.9249 - gp: 4.1582\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5252 - d_loss: -78.6608 - g_loss: 46.9370 - gp: 3.6269\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5531 - d_loss: -74.1229 - g_loss: 48.0371 - gp: 3.8393\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5392 - d_loss: -76.2630 - g_loss: 37.5007 - gp: 3.3189\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5823 - d_loss: -74.0474 - g_loss: 47.7522 - gp: 3.5980\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5936 - d_loss: -76.7236 - g_loss: 55.1901 - gp: 3.7671\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5889 - d_loss: -81.7662 - g_loss: 57.6942 - gp: 3.5969\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5915 - d_loss: -73.3627 - g_loss: 56.7833 - gp: 3.5163\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5723 - d_loss: -70.7138 - g_loss: 47.5584 - gp: 3.6235\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5554 - d_loss: -76.3439 - g_loss: 53.5588 - gp: 3.4232\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5227 - d_loss: -74.7104 - g_loss: 47.7451 - gp: 3.8051\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5157 - d_loss: -79.3239 - g_loss: 46.7651 - gp: 3.6089\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5524 - d_loss: -78.4438 - g_loss: 39.1379 - gp: 3.7176\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5182 - d_loss: -79.4576 - g_loss: 43.2057 - gp: 3.6264\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5306 - d_loss: -77.1995 - g_loss: 43.1299 - gp: 3.6054\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5655 - d_loss: -77.4239 - g_loss: 44.2456 - gp: 3.6259\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5742 - d_loss: -73.2627 - g_loss: 52.5979 - gp: 3.4972\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5895 - d_loss: -73.2353 - g_loss: 61.3635 - gp: 3.5775\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5846 - d_loss: -73.4353 - g_loss: 49.3168 - gp: 3.7055\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5811 - d_loss: -79.8558 - g_loss: 53.6859 - gp: 3.7329\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5384 - d_loss: -75.0911 - g_loss: 55.0889 - gp: 3.8839\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5535 - d_loss: -78.4269 - g_loss: 55.4823 - gp: 3.7431\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5680 - d_loss: -73.0285 - g_loss: 53.3968 - gp: 3.7167\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5712 - d_loss: -74.7035 - g_loss: 51.1203 - gp: 3.4674\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5527 - d_loss: -81.3146 - g_loss: 46.7779 - gp: 3.4582\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5433 - d_loss: -78.3239 - g_loss: 50.1717 - gp: 3.6305\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5368 - d_loss: -75.1656 - g_loss: 58.2674 - gp: 3.8292\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.4997 - d_loss: -79.0333 - g_loss: 72.3541 - gp: 3.6533\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5606 - d_loss: -75.4225 - g_loss: 55.9216 - gp: 3.6988\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5769 - d_loss: -76.1867 - g_loss: 56.1736 - gp: 3.8193\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5593 - d_loss: -78.4257 - g_loss: 59.1798 - gp: 3.7297\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5852 - d_loss: -71.1262 - g_loss: 58.8352 - gp: 3.7234\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5935 - d_loss: -69.3760 - g_loss: 53.7092 - gp: 3.8119\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5827 - d_loss: -82.9250 - g_loss: 62.0113 - gp: 3.3960\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5697 - d_loss: -72.2002 - g_loss: 57.8535 - gp: 3.7605\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5883 - d_loss: -75.9446 - g_loss: 59.9148 - gp: 3.6104\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5748 - d_loss: -76.7160 - g_loss: 55.5339 - gp: 3.7443\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5632 - d_loss: -74.2480 - g_loss: 54.8263 - gp: 3.8256\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5295 - d_loss: -75.1577 - g_loss: 60.1192 - gp: 3.7583\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5619 - d_loss: -76.1179 - g_loss: 65.5084 - gp: 3.5315\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5704 - d_loss: -83.0003 - g_loss: 78.1127 - gp: 3.9162\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5692 - d_loss: -73.9347 - g_loss: 53.3238 - gp: 3.7723\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5783 - d_loss: -76.8827 - g_loss: 59.9886 - gp: 3.6959\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5651 - d_loss: -78.6734 - g_loss: 68.0419 - gp: 3.8787\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5744 - d_loss: -80.0549 - g_loss: 50.7395 - gp: 3.6240\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5802 - d_loss: -72.3297 - g_loss: 60.2184 - gp: 4.0010\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5934 - d_loss: -72.3259 - g_loss: 57.4205 - gp: 3.5451\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5858 - d_loss: -75.0925 - g_loss: 60.1746 - gp: 3.6346\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5912 - d_loss: -78.6189 - g_loss: 68.2404 - gp: 3.8586\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5850 - d_loss: -82.1256 - g_loss: 49.9713 - gp: 3.5582\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5712 - d_loss: -75.7223 - g_loss: 52.5089 - gp: 3.6218\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5872 - d_loss: -73.7240 - g_loss: 52.7291 - gp: 3.8228\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5359 - d_loss: -75.3117 - g_loss: 49.9114 - gp: 3.8365\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5360 - d_loss: -76.0495 - g_loss: 52.8145 - gp: 3.6598\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5932 - d_loss: -77.6703 - g_loss: 64.2113 - gp: 3.7757\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5974 - d_loss: -74.8096 - g_loss: 56.1499 - gp: 3.8144\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5979 - d_loss: -74.2936 - g_loss: 37.9511 - gp: 3.4435\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6050 - d_loss: -79.0170 - g_loss: 44.9014 - gp: 3.5878\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5964 - d_loss: -75.1823 - g_loss: 55.7681 - gp: 3.8195\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 1s 505ms/step - accuracy: 0.6056 - d_loss: -79.8130 - g_loss: 61.1416 - gp: 3.9956\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5880 - d_loss: -81.5508 - g_loss: 50.4085 - gp: 3.8667\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5701 - d_loss: -69.6512 - g_loss: 34.6247 - gp: 4.0762\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5549 - d_loss: -85.2457 - g_loss: 43.0108 - gp: 3.6292\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5496 - d_loss: -77.6187 - g_loss: 48.6763 - gp: 3.9526\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5524 - d_loss: -73.2680 - g_loss: 62.8187 - gp: 3.8599\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5983 - d_loss: -85.5123 - g_loss: 40.0766 - gp: 3.2039\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6067 - d_loss: -77.0586 - g_loss: 41.7978 - gp: 3.9322\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5868 - d_loss: -75.7757 - g_loss: 52.1535 - gp: 3.8627\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6003 - d_loss: -75.2144 - g_loss: 61.2240 - gp: 3.9940\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5964 - d_loss: -76.6114 - g_loss: 49.1921 - gp: 3.7924\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5860 - d_loss: -77.7659 - g_loss: 44.0219 - gp: 3.5396\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5728 - d_loss: -75.4947 - g_loss: 45.4405 - gp: 3.7636\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5660 - d_loss: -73.1050 - g_loss: 47.6676 - gp: 3.7333\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5794 - d_loss: -76.8354 - g_loss: 42.5075 - gp: 3.4436\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5533 - d_loss: -73.7744 - g_loss: 36.5724 - gp: 3.8448\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5467 - d_loss: -71.4640 - g_loss: 45.3159 - gp: 3.7399\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5830 - d_loss: -77.5911 - g_loss: 38.2472 - gp: 3.5653\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5780 - d_loss: -78.7673 - g_loss: 58.8081 - gp: 3.6979\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5837 - d_loss: -75.5962 - g_loss: 44.6000 - gp: 3.5414\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5833 - d_loss: -75.8175 - g_loss: 50.7616 - gp: 3.6276\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 1s 509ms/step - accuracy: 0.5728 - d_loss: -72.9035 - g_loss: 38.4247 - gp: 3.5775\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5695 - d_loss: -76.2728 - g_loss: 40.9543 - gp: 3.7053\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5739 - d_loss: -75.7024 - g_loss: 47.5841 - gp: 4.0007\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5890 - d_loss: -70.4915 - g_loss: 46.2391 - gp: 3.8109\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5678 - d_loss: -77.3860 - g_loss: 44.8207 - gp: 3.4399\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5482 - d_loss: -78.9138 - g_loss: 38.5701 - gp: 3.5732\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5358 - d_loss: -73.4872 - g_loss: 42.7559 - gp: 3.6356\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5553 - d_loss: -79.5293 - g_loss: 38.2921 - gp: 3.6054\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5652 - d_loss: -80.2890 - g_loss: 47.3793 - gp: 3.6036\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5805 - d_loss: -71.8995 - g_loss: 42.8575 - gp: 4.0253\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5683 - d_loss: -69.8350 - g_loss: 51.6269 - gp: 3.8340\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5851 - d_loss: -76.5723 - g_loss: 53.1843 - gp: 3.3679\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5971 - d_loss: -72.9598 - g_loss: 28.6081 - gp: 3.5425\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5823 - d_loss: -75.4880 - g_loss: 44.7485 - gp: 3.5800\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5687 - d_loss: -73.7729 - g_loss: 44.8543 - gp: 3.5849\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5815 - d_loss: -74.6993 - g_loss: 40.2411 - gp: 3.6443\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5346 - d_loss: -76.7744 - g_loss: 53.4786 - gp: 3.9085\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5890 - d_loss: -73.7292 - g_loss: 34.3339 - gp: 3.5615\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5891 - d_loss: -72.1530 - g_loss: 40.5856 - gp: 3.5509\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5940 - d_loss: -77.1443 - g_loss: 53.4239 - gp: 3.5685\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5838 - d_loss: -72.7365 - g_loss: 36.9853 - gp: 3.6884\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 1s 501ms/step - accuracy: 0.5844 - d_loss: -82.3239 - g_loss: 48.3746 - gp: 3.5329\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 1s 509ms/step - accuracy: 0.5733 - d_loss: -76.8079 - g_loss: 50.6763 - gp: 3.8305\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5747 - d_loss: -75.2886 - g_loss: 40.6394 - gp: 3.3545\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5649 - d_loss: -74.9285 - g_loss: 48.4104 - gp: 3.4160\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5816 - d_loss: -74.9487 - g_loss: 42.2101 - gp: 3.6684\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5982 - d_loss: -74.9344 - g_loss: 38.8661 - gp: 3.7693\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5674 - d_loss: -70.8540 - g_loss: 54.3990 - gp: 4.0230\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5878 - d_loss: -76.3808 - g_loss: 55.1374 - gp: 3.6548\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5453 - d_loss: -69.8972 - g_loss: 36.3057 - gp: 3.6841\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5751 - d_loss: -74.2599 - g_loss: 54.2804 - gp: 3.6896\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5928 - d_loss: -73.6543 - g_loss: 34.9091 - gp: 3.2655\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.6025 - d_loss: -73.2106 - g_loss: 33.0375 - gp: 3.4331\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5737 - d_loss: -74.9610 - g_loss: 42.4703 - gp: 3.4333\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5671 - d_loss: -75.0370 - g_loss: 51.9232 - gp: 3.7291\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5909 - d_loss: -76.7396 - g_loss: 55.3376 - gp: 3.6522\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5859 - d_loss: -75.3498 - g_loss: 50.5340 - gp: 3.7623\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5770 - d_loss: -79.0411 - g_loss: 59.4824 - gp: 3.7354\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5967 - d_loss: -71.5934 - g_loss: 43.6526 - gp: 3.5587\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5828 - d_loss: -74.8855 - g_loss: 46.7164 - gp: 3.5757\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5931 - d_loss: -72.6888 - g_loss: 47.2918 - gp: 3.7344\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 500ms/step - accuracy: 0.5856 - d_loss: -74.0471 - g_loss: 56.4357 - gp: 3.5721\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 1s 505ms/step - accuracy: 0.6059 - d_loss: -78.4515 - g_loss: 60.5004 - gp: 3.7850\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5798 - d_loss: -83.5477 - g_loss: 52.1231 - gp: 3.5954\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5719 - d_loss: -73.4372 - g_loss: 47.7138 - gp: 3.8031\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5886 - d_loss: -82.7795 - g_loss: 56.3167 - gp: 3.4686\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6072 - d_loss: -71.1790 - g_loss: 38.8262 - gp: 3.8809\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5640 - d_loss: -71.5866 - g_loss: 59.5305 - gp: 3.7754\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6161 - d_loss: -82.1538 - g_loss: 69.7866 - gp: 3.6583\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5741 - d_loss: -77.7355 - g_loss: 51.2249 - gp: 3.6800\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5779 - d_loss: -73.4902 - g_loss: 56.1248 - gp: 3.7291\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5980 - d_loss: -71.7958 - g_loss: 46.8528 - gp: 3.5606\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.6021 - d_loss: -75.8156 - g_loss: 51.2332 - gp: 3.4968\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6001 - d_loss: -70.9647 - g_loss: 52.6174 - gp: 3.9154\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5810 - d_loss: -71.5440 - g_loss: 61.2283 - gp: 3.8683\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5803 - d_loss: -72.7802 - g_loss: 59.9316 - gp: 3.6463\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5801 - d_loss: -72.4626 - g_loss: 56.0596 - gp: 3.6337\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5660 - d_loss: -70.9338 - g_loss: 47.3761 - gp: 3.5540\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5729 - d_loss: -71.0617 - g_loss: 55.6947 - gp: 3.6134\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5651 - d_loss: -77.9895 - g_loss: 31.5633 - gp: 3.1149\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5656 - d_loss: -76.7219 - g_loss: 64.5787 - gp: 3.7289\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5468 - d_loss: -68.1702 - g_loss: 49.1882 - gp: 3.7569\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5775 - d_loss: -74.0745 - g_loss: 53.6813 - gp: 3.4644\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5910 - d_loss: -74.7143 - g_loss: 55.3665 - gp: 3.8782\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5660 - d_loss: -77.5630 - g_loss: 50.7227 - gp: 3.7844\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5957 - d_loss: -74.3396 - g_loss: 50.7877 - gp: 3.9518\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5456 - d_loss: -78.2507 - g_loss: 43.0350 - gp: 3.3224\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 475ms/step - accuracy: 0.5633 - d_loss: -73.0640 - g_loss: 35.6731 - gp: 3.5477\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5980 - d_loss: -71.2295 - g_loss: 58.4938 - gp: 3.6197\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5802 - d_loss: -75.8413 - g_loss: 45.4560 - gp: 3.7134\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5964 - d_loss: -68.3968 - g_loss: 49.5599 - gp: 3.8639\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5680 - d_loss: -81.8106 - g_loss: 63.6237 - gp: 3.7946\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5832 - d_loss: -74.6485 - g_loss: 46.8028 - gp: 3.7188\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5553 - d_loss: -76.3944 - g_loss: 44.3278 - gp: 3.5253\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5821 - d_loss: -74.5668 - g_loss: 44.9883 - gp: 3.5535\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5751 - d_loss: -78.3002 - g_loss: 41.5485 - gp: 3.4346\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6069 - d_loss: -73.9296 - g_loss: 52.0751 - gp: 3.8202\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6075 - d_loss: -72.3547 - g_loss: 49.6407 - gp: 3.7303\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5799 - d_loss: -71.7643 - g_loss: 52.6706 - gp: 3.5379\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5833 - d_loss: -75.1502 - g_loss: 46.5389 - gp: 3.5079\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5846 - d_loss: -70.3352 - g_loss: 44.6451 - gp: 3.7498\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5916 - d_loss: -68.8931 - g_loss: 44.9003 - gp: 3.7971\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5707 - d_loss: -74.2424 - g_loss: 37.1268 - gp: 3.2440\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.5830 - d_loss: -72.3745 - g_loss: 38.2179 - gp: 3.4181\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5891 - d_loss: -77.0619 - g_loss: 50.0270 - gp: 3.3601\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5743 - d_loss: -67.7185 - g_loss: 39.2656 - gp: 3.6843\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5300 - d_loss: -74.6521 - g_loss: 54.0731 - gp: 3.2849\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5687 - d_loss: -70.3480 - g_loss: 52.7581 - gp: 3.6968\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5874 - d_loss: -68.0904 - g_loss: 48.0751 - gp: 3.3480\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5927 - d_loss: -73.4148 - g_loss: 62.7015 - gp: 3.3591\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5825 - d_loss: -68.5502 - g_loss: 24.8559 - gp: 3.1097\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6098 - d_loss: -66.0178 - g_loss: 36.5128 - gp: 3.2878\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6005 - d_loss: -75.3445 - g_loss: 48.5816 - gp: 3.4742\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5863 - d_loss: -75.9162 - g_loss: 52.0488 - gp: 3.5639\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.6027 - d_loss: -72.9520 - g_loss: 42.0612 - gp: 3.5705\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5855 - d_loss: -69.6169 - g_loss: 46.3715 - gp: 3.4880\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5932 - d_loss: -78.6988 - g_loss: 55.2951 - gp: 3.6220\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5358 - d_loss: -73.5101 - g_loss: 42.1964 - gp: 3.4137\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5600 - d_loss: -71.7435 - g_loss: 40.6554 - gp: 3.5042\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5727 - d_loss: -68.8510 - g_loss: 38.2479 - gp: 3.1835\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5477 - d_loss: -71.9339 - g_loss: 61.3531 - gp: 3.3455\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5574 - d_loss: -74.1201 - g_loss: 62.2818 - gp: 3.2943\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5644 - d_loss: -71.3799 - g_loss: 56.4484 - gp: 3.4858\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6054 - d_loss: -78.3136 - g_loss: 56.8104 - gp: 3.5426\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5543 - d_loss: -73.3441 - g_loss: 46.9686 - gp: 3.8979\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5370 - d_loss: -74.1607 - g_loss: 38.5072 - gp: 3.4984\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 1s 501ms/step - accuracy: 0.5519 - d_loss: -72.7400 - g_loss: 56.6034 - gp: 3.6978\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5984 - d_loss: -73.5355 - g_loss: 50.9337 - gp: 3.3456\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6042 - d_loss: -76.8103 - g_loss: 46.7631 - gp: 3.3220\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5844 - d_loss: -74.7795 - g_loss: 53.3251 - gp: 3.3388\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5898 - d_loss: -79.1020 - g_loss: 57.3432 - gp: 3.4727\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5914 - d_loss: -69.8970 - g_loss: 55.0515 - gp: 3.7616\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5474 - d_loss: -74.5785 - g_loss: 54.6642 - gp: 3.4870\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5805 - d_loss: -74.2665 - g_loss: 56.1747 - gp: 3.7212\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5471 - d_loss: -68.5172 - g_loss: 39.6276 - gp: 3.5787\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5558 - d_loss: -77.6596 - g_loss: 65.3959 - gp: 3.6453\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5672 - d_loss: -70.5762 - g_loss: 57.5031 - gp: 3.4612\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5957 - d_loss: -72.0094 - g_loss: 54.0009 - gp: 3.2039\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5927 - d_loss: -72.4355 - g_loss: 52.0794 - gp: 3.3822\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.6006 - d_loss: -71.7528 - g_loss: 45.0991 - gp: 3.5210\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5881 - d_loss: -73.3227 - g_loss: 55.4532 - gp: 3.6896\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5959 - d_loss: -78.8088 - g_loss: 58.4238 - gp: 3.5774\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5676 - d_loss: -75.8383 - g_loss: 51.8558 - gp: 3.6019\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5485 - d_loss: -78.5899 - g_loss: 52.4517 - gp: 3.3478\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5755 - d_loss: -75.4774 - g_loss: 57.3311 - gp: 3.6741\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5768 - d_loss: -75.0751 - g_loss: 52.2391 - gp: 3.5516\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5736 - d_loss: -71.6071 - g_loss: 51.2777 - gp: 3.8952\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.6007 - d_loss: -76.3660 - g_loss: 64.1450 - gp: 3.5369\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5909 - d_loss: -71.4246 - g_loss: 67.9395 - gp: 3.4595\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5923 - d_loss: -70.0183 - g_loss: 48.6440 - gp: 3.5422\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5961 - d_loss: -78.2230 - g_loss: 59.1133 - gp: 3.4494\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5915 - d_loss: -76.9546 - g_loss: 55.9343 - gp: 3.4875\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5766 - d_loss: -73.4815 - g_loss: 53.4702 - gp: 3.6476\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5720 - d_loss: -74.2211 - g_loss: 61.5758 - gp: 3.7320\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5977 - d_loss: -77.7364 - g_loss: 64.8100 - gp: 3.8704\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5879 - d_loss: -72.9464 - g_loss: 51.1095 - gp: 3.4801\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5595 - d_loss: -72.3420 - g_loss: 52.9894 - gp: 3.4025\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6074 - d_loss: -72.6996 - g_loss: 55.5501 - gp: 3.5358\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6030 - d_loss: -72.3436 - g_loss: 61.7838 - gp: 3.5737\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6116 - d_loss: -69.4636 - g_loss: 52.3414 - gp: 3.4958\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6101 - d_loss: -77.9684 - g_loss: 68.3588 - gp: 3.5694\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6005 - d_loss: -70.6511 - g_loss: 54.9106 - gp: 3.8668\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5815 - d_loss: -72.8884 - g_loss: 58.9749 - gp: 3.6088\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6031 - d_loss: -76.7860 - g_loss: 70.8208 - gp: 3.7490\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6062 - d_loss: -75.6449 - g_loss: 54.6145 - gp: 3.5347\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5690 - d_loss: -74.6191 - g_loss: 59.7910 - gp: 3.5821\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5632 - d_loss: -71.8846 - g_loss: 50.3901 - gp: 3.5796\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 1s 505ms/step - accuracy: 0.5947 - d_loss: -79.4446 - g_loss: 66.9398 - gp: 3.5763\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 1s 501ms/step - accuracy: 0.5917 - d_loss: -72.6835 - g_loss: 57.5603 - gp: 3.7279\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 1s 516ms/step - accuracy: 0.6178 - d_loss: -75.5890 - g_loss: 54.2036 - gp: 3.5705\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6181 - d_loss: -70.9190 - g_loss: 46.7775 - gp: 3.7598\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5937 - d_loss: -79.9378 - g_loss: 68.8631 - gp: 3.8303\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5919 - d_loss: -74.5535 - g_loss: 53.5380 - gp: 3.7533\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5936 - d_loss: -77.2903 - g_loss: 63.2507 - gp: 3.7141\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6059 - d_loss: -79.6041 - g_loss: 59.4543 - gp: 3.9228\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5722 - d_loss: -75.1494 - g_loss: 51.1197 - gp: 3.8053\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5670 - d_loss: -67.9101 - g_loss: 57.1956 - gp: 3.8184\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5758 - d_loss: -68.8376 - g_loss: 59.2860 - gp: 3.5175\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5962 - d_loss: -75.0234 - g_loss: 56.3831 - gp: 3.4158\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6011 - d_loss: -85.9556 - g_loss: 85.7670 - gp: 3.8365\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6058 - d_loss: -75.3644 - g_loss: 47.5195 - gp: 3.6124\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5895 - d_loss: -79.6485 - g_loss: 34.7464 - gp: 3.3982\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.6037 - d_loss: -69.3609 - g_loss: 50.9648 - gp: 3.8418\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5891 - d_loss: -72.8406 - g_loss: 60.5424 - gp: 3.6196\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.6225 - d_loss: -79.5444 - g_loss: 58.3766 - gp: 3.8629\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5836 - d_loss: -85.0076 - g_loss: 73.2432 - gp: 3.9215\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5901 - d_loss: -78.2421 - g_loss: 43.8933 - gp: 3.7550\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5565 - d_loss: -72.4892 - g_loss: 44.3141 - gp: 3.7448\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5675 - d_loss: -80.8299 - g_loss: 60.7988 - gp: 3.5898\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 1s 509ms/step - accuracy: 0.6135 - d_loss: -73.9457 - g_loss: 50.6949 - gp: 3.8478\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 1s 513ms/step - accuracy: 0.6161 - d_loss: -81.4440 - g_loss: 55.0259 - gp: 3.4619\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 1s 513ms/step - accuracy: 0.5936 - d_loss: -78.2761 - g_loss: 41.1730 - gp: 3.6818\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6034 - d_loss: -74.5964 - g_loss: 57.4749 - gp: 3.7423\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5920 - d_loss: -68.2283 - g_loss: 44.5071 - gp: 3.7822\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6033 - d_loss: -69.1077 - g_loss: 57.5925 - gp: 3.8930\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.6078 - d_loss: -80.3052 - g_loss: 59.7758 - gp: 3.6438\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5836 - d_loss: -73.1842 - g_loss: 58.7488 - gp: 3.8451\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5898 - d_loss: -76.4433 - g_loss: 50.4569 - gp: 3.2574\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5785 - d_loss: -72.5619 - g_loss: 31.1528 - gp: 3.4653\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5965 - d_loss: -73.1877 - g_loss: 43.6927 - gp: 3.5130\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5981 - d_loss: -70.3951 - g_loss: 44.6499 - gp: 3.5302\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5915 - d_loss: -76.4309 - g_loss: 67.3621 - gp: 3.5074\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5835 - d_loss: -72.7666 - g_loss: 43.9370 - gp: 3.2923\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5846 - d_loss: -69.0695 - g_loss: 47.2497 - gp: 3.6691\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5894 - d_loss: -73.4660 - g_loss: 56.7353 - gp: 3.7098\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5989 - d_loss: -73.2958 - g_loss: 57.2201 - gp: 3.7905\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5742 - d_loss: -72.9527 - g_loss: 50.7043 - gp: 3.6133\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5444 - d_loss: -73.4294 - g_loss: 49.0759 - gp: 3.4396\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6120 - d_loss: -73.4567 - g_loss: 44.1118 - gp: 3.5465\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6084 - d_loss: -72.8710 - g_loss: 54.9980 - gp: 3.5135\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.6071 - d_loss: -81.0289 - g_loss: 54.1640 - gp: 3.7596\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 1s 523ms/step - accuracy: 0.5796 - d_loss: -70.4737 - g_loss: 48.1745 - gp: 3.5405\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 1s 508ms/step - accuracy: 0.5736 - d_loss: -70.1598 - g_loss: 46.0975 - gp: 3.6736\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5771 - d_loss: -78.6356 - g_loss: 56.5022 - gp: 3.5291\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5681 - d_loss: -73.2164 - g_loss: 34.2430 - gp: 3.3966\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5850 - d_loss: -72.9240 - g_loss: 69.0115 - gp: 3.7129\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5796 - d_loss: -69.4112 - g_loss: 37.4763 - gp: 3.2937\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5896 - d_loss: -65.4548 - g_loss: 48.2279 - gp: 3.4021\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5961 - d_loss: -72.0124 - g_loss: 45.8052 - gp: 3.2595\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5928 - d_loss: -74.9351 - g_loss: 54.3340 - gp: 3.5978\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5708 - d_loss: -69.9737 - g_loss: 50.6944 - gp: 3.5539\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5701 - d_loss: -70.0664 - g_loss: 49.8851 - gp: 3.3180\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5602 - d_loss: -73.7593 - g_loss: 64.6991 - gp: 3.1947\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5824 - d_loss: -72.9474 - g_loss: 45.2465 - gp: 3.4223\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5895 - d_loss: -72.3270 - g_loss: 48.7002 - gp: 3.5474\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5993 - d_loss: -71.0230 - g_loss: 47.1506 - gp: 3.8051\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5826 - d_loss: -68.7167 - g_loss: 50.7640 - gp: 3.6438\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6009 - d_loss: -74.4579 - g_loss: 57.5737 - gp: 3.6951\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5874 - d_loss: -72.5380 - g_loss: 59.1956 - gp: 3.4418\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5644 - d_loss: -72.4793 - g_loss: 49.9901 - gp: 3.3384\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5678 - d_loss: -69.0616 - g_loss: 46.9419 - gp: 3.1202\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5590 - d_loss: -71.6325 - g_loss: 44.7376 - gp: 3.1841\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5883 - d_loss: -71.3509 - g_loss: 46.5747 - gp: 3.2436\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5311 - d_loss: -73.0374 - g_loss: 52.2634 - gp: 3.3604\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5375 - d_loss: -67.1115 - g_loss: 52.5577 - gp: 3.5623\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5615 - d_loss: -68.0063 - g_loss: 50.0844 - gp: 3.2237\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5468 - d_loss: -65.1767 - g_loss: 43.4187 - gp: 3.3619\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5926 - d_loss: -68.2313 - g_loss: 45.3642 - gp: 3.0329\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6004 - d_loss: -71.3691 - g_loss: 54.1853 - gp: 3.2083\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6041 - d_loss: -70.1114 - g_loss: 60.9894 - gp: 3.4114\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6157 - d_loss: -70.9633 - g_loss: 64.6797 - gp: 3.2131\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5763 - d_loss: -68.5013 - g_loss: 69.2455 - gp: 3.5860\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5819 - d_loss: -74.4404 - g_loss: 61.2012 - gp: 3.4319\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5702 - d_loss: -73.0803 - g_loss: 53.4300 - gp: 3.4073\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5544 - d_loss: -74.2766 - g_loss: 60.4357 - gp: 3.2658\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5850 - d_loss: -68.1960 - g_loss: 67.4959 - gp: 3.2059\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5777 - d_loss: -69.7491 - g_loss: 60.5495 - gp: 3.0767\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5640 - d_loss: -67.8902 - g_loss: 54.2462 - gp: 3.0865\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5768 - d_loss: -68.4650 - g_loss: 60.1148 - gp: 3.2864\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5793 - d_loss: -69.6121 - g_loss: 58.1464 - gp: 3.3954\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5606 - d_loss: -68.5530 - g_loss: 58.5539 - gp: 3.2801\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5451 - d_loss: -68.7251 - g_loss: 52.7030 - gp: 3.2256\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5691 - d_loss: -68.4306 - g_loss: 57.5095 - gp: 3.2504\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 1s 501ms/step - accuracy: 0.5756 - d_loss: -69.6581 - g_loss: 63.0012 - gp: 3.1986\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5898 - d_loss: -67.0545 - g_loss: 59.4641 - gp: 3.1997\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5816 - d_loss: -67.8489 - g_loss: 51.0801 - gp: 3.1256\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5977 - d_loss: -68.4939 - g_loss: 60.0067 - gp: 3.4598\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6004 - d_loss: -67.5520 - g_loss: 62.9240 - gp: 3.2925\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6012 - d_loss: -71.2175 - g_loss: 67.6617 - gp: 3.3293\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5501 - d_loss: -68.9756 - g_loss: 47.5382 - gp: 3.0633\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5645 - d_loss: -69.0443 - g_loss: 52.8719 - gp: 3.1242\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5506 - d_loss: -69.1959 - g_loss: 50.2289 - gp: 3.3655\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5586 - d_loss: -69.7900 - g_loss: 63.1392 - gp: 3.1704\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5890 - d_loss: -74.5622 - g_loss: 73.5456 - gp: 3.6135\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5516 - d_loss: -67.7924 - g_loss: 33.5540 - gp: 3.0252\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5659 - d_loss: -67.4978 - g_loss: 43.8490 - gp: 3.1148\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6008 - d_loss: -69.5651 - g_loss: 54.9175 - gp: 3.2762\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5699 - d_loss: -66.7613 - g_loss: 53.9706 - gp: 3.4693\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5899 - d_loss: -65.9365 - g_loss: 66.8591 - gp: 3.2090\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5859 - d_loss: -67.1461 - g_loss: 51.2322 - gp: 3.3090\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5442 - d_loss: -68.9119 - g_loss: 64.3930 - gp: 3.3469\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5469 - d_loss: -67.9292 - g_loss: 66.0813 - gp: 3.4712\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5899 - d_loss: -72.2775 - g_loss: 65.8905 - gp: 3.4065\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5839 - d_loss: -73.6220 - g_loss: 61.4850 - gp: 3.2647\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5600 - d_loss: -70.5253 - g_loss: 56.4953 - gp: 3.0992\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5706 - d_loss: -69.4262 - g_loss: 58.6493 - gp: 3.1936\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5928 - d_loss: -71.9134 - g_loss: 58.3802 - gp: 3.0508\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5706 - d_loss: -67.5115 - g_loss: 59.8018 - gp: 3.4195\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5216 - d_loss: -69.6579 - g_loss: 75.8494 - gp: 3.2668\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5848 - d_loss: -71.7791 - g_loss: 77.3073 - gp: 3.3742\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5436 - d_loss: -68.5778 - g_loss: 61.6361 - gp: 2.9998\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5919 - d_loss: -70.5729 - g_loss: 58.4235 - gp: 3.4149\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5842 - d_loss: -70.5305 - g_loss: 61.0981 - gp: 3.3156\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5698 - d_loss: -65.7441 - g_loss: 61.7328 - gp: 3.2354\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 476ms/step - accuracy: 0.5956 - d_loss: -71.7143 - g_loss: 67.8037 - gp: 3.2365\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5803 - d_loss: -69.3930 - g_loss: 67.0910 - gp: 3.2643\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5652 - d_loss: -65.0187 - g_loss: 63.1637 - gp: 3.1461\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5475 - d_loss: -69.0233 - g_loss: 67.9409 - gp: 2.9034\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5564 - d_loss: -68.1160 - g_loss: 68.7334 - gp: 3.3106\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5661 - d_loss: -73.5975 - g_loss: 68.1673 - gp: 3.3666\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5679 - d_loss: -69.6919 - g_loss: 71.5069 - gp: 3.3358\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5701 - d_loss: -69.0985 - g_loss: 68.4888 - gp: 3.1349\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5640 - d_loss: -71.6494 - g_loss: 63.4436 - gp: 3.1050\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5714 - d_loss: -70.1695 - g_loss: 53.0526 - gp: 3.1778\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5688 - d_loss: -69.0246 - g_loss: 62.9370 - gp: 3.0025\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5844 - d_loss: -68.3378 - g_loss: 72.5148 - gp: 3.2976\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5648 - d_loss: -68.6216 - g_loss: 67.6061 - gp: 3.4470\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5470 - d_loss: -67.7006 - g_loss: 75.2743 - gp: 3.5114\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5606 - d_loss: -72.8444 - g_loss: 72.0310 - gp: 3.3820\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5660 - d_loss: -68.4247 - g_loss: 68.3899 - gp: 3.1850\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5884 - d_loss: -70.9345 - g_loss: 67.8742 - gp: 3.4921\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5702 - d_loss: -66.7324 - g_loss: 66.7720 - gp: 3.0790\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5799 - d_loss: -69.8037 - g_loss: 71.9958 - gp: 3.2634\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5803 - d_loss: -70.8053 - g_loss: 63.2887 - gp: 3.2189\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5448 - d_loss: -70.9445 - g_loss: 76.1308 - gp: 3.3499\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5518 - d_loss: -71.4077 - g_loss: 69.0511 - gp: 3.3798\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5629 - d_loss: -69.7313 - g_loss: 69.9827 - gp: 3.3921\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5412 - d_loss: -68.6844 - g_loss: 62.5020 - gp: 3.1552\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5318 - d_loss: -74.8934 - g_loss: 76.6014 - gp: 3.2863\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5233 - d_loss: -70.8331 - g_loss: 56.8655 - gp: 3.2875\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5958 - d_loss: -68.8741 - g_loss: 62.2761 - gp: 3.4267\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5812 - d_loss: -67.7671 - g_loss: 56.8222 - gp: 3.0356\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5618 - d_loss: -69.2482 - g_loss: 58.4876 - gp: 3.3042\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5862 - d_loss: -66.8813 - g_loss: 68.9987 - gp: 3.3539\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5702 - d_loss: -72.8663 - g_loss: 61.2952 - gp: 3.2297\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5436 - d_loss: -75.5161 - g_loss: 52.8268 - gp: 3.2364\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5695 - d_loss: -71.0618 - g_loss: 67.6023 - gp: 3.5585\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 1s 508ms/step - accuracy: 0.5591 - d_loss: -65.6680 - g_loss: 57.2462 - gp: 3.2581\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5630 - d_loss: -68.5835 - g_loss: 58.9162 - gp: 2.9144\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5446 - d_loss: -71.2821 - g_loss: 58.2347 - gp: 3.0939\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5551 - d_loss: -71.5029 - g_loss: 57.2734 - gp: 3.4296\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5148 - d_loss: -68.4543 - g_loss: 67.7784 - gp: 3.3788\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5498 - d_loss: -69.6588 - g_loss: 57.0370 - gp: 3.2740\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5673 - d_loss: -69.7277 - g_loss: 59.1071 - gp: 3.2726\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5329 - d_loss: -69.7024 - g_loss: 56.6003 - gp: 3.1691\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5381 - d_loss: -72.4327 - g_loss: 73.1604 - gp: 3.3027\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5452 - d_loss: -70.9115 - g_loss: 70.7717 - gp: 3.0343\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5207 - d_loss: -75.9834 - g_loss: 60.1792 - gp: 3.0955\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5362 - d_loss: -70.3733 - g_loss: 71.5813 - gp: 3.3727\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5334 - d_loss: -70.3985 - g_loss: 78.3583 - gp: 3.4543\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5545 - d_loss: -68.0697 - g_loss: 66.2595 - gp: 3.2889\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5282 - d_loss: -72.5552 - g_loss: 86.0161 - gp: 3.2792\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5194 - d_loss: -67.7224 - g_loss: 66.2504 - gp: 3.1451\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5680 - d_loss: -71.1468 - g_loss: 72.6619 - gp: 3.2339\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5638 - d_loss: -69.4196 - g_loss: 73.1272 - gp: 3.3387\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5872 - d_loss: -70.6585 - g_loss: 82.2017 - gp: 3.3042\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5547 - d_loss: -72.0533 - g_loss: 69.8980 - gp: 3.0172\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5380 - d_loss: -73.2675 - g_loss: 83.8745 - gp: 3.3008\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5712 - d_loss: -70.3837 - g_loss: 74.8978 - gp: 3.2467\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 1s 518ms/step - accuracy: 0.5584 - d_loss: -69.4206 - g_loss: 76.2118 - gp: 3.4179\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5370 - d_loss: -72.1261 - g_loss: 83.6020 - gp: 3.2812\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5551 - d_loss: -68.3985 - g_loss: 68.7178 - gp: 3.6403\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5432 - d_loss: -69.3370 - g_loss: 74.9676 - gp: 3.4084\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5448 - d_loss: -72.3024 - g_loss: 78.4614 - gp: 3.3831\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5871 - d_loss: -75.3535 - g_loss: 73.5308 - gp: 3.2737\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5624 - d_loss: -70.7435 - g_loss: 74.2747 - gp: 3.1564\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5750 - d_loss: -72.6128 - g_loss: 79.3557 - gp: 3.3562\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5781 - d_loss: -73.2433 - g_loss: 80.0364 - gp: 3.3093\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5886 - d_loss: -69.5011 - g_loss: 70.7714 - gp: 3.4177\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5502 - d_loss: -72.4645 - g_loss: 79.9235 - gp: 3.0973\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5713 - d_loss: -72.2780 - g_loss: 73.9889 - gp: 3.4044\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5473 - d_loss: -70.4026 - g_loss: 86.8969 - gp: 3.4593\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5539 - d_loss: -71.5988 - g_loss: 77.9597 - gp: 3.2647\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5786 - d_loss: -72.6684 - g_loss: 68.9719 - gp: 3.2334\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5703 - d_loss: -74.3676 - g_loss: 73.4531 - gp: 3.3373\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5446 - d_loss: -70.3771 - g_loss: 77.8401 - gp: 3.1931\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5578 - d_loss: -70.0432 - g_loss: 82.6082 - gp: 3.4167\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5350 - d_loss: -72.2849 - g_loss: 82.0218 - gp: 3.6842\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5766 - d_loss: -71.2578 - g_loss: 79.3284 - gp: 3.3500\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6044 - d_loss: -74.4670 - g_loss: 81.7005 - gp: 3.4063\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 1s 513ms/step - accuracy: 0.5851 - d_loss: -71.7561 - g_loss: 88.1864 - gp: 3.4444\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 1s 515ms/step - accuracy: 0.5999 - d_loss: -75.0238 - g_loss: 70.2226 - gp: 3.3823\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5850 - d_loss: -76.1321 - g_loss: 82.5545 - gp: 3.6847\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5911 - d_loss: -70.6494 - g_loss: 79.6834 - gp: 3.7217\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5875 - d_loss: -73.1507 - g_loss: 73.9812 - gp: 3.4710\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5957 - d_loss: -76.7815 - g_loss: 93.6406 - gp: 3.6113\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5921 - d_loss: -70.1667 - g_loss: 75.4288 - gp: 3.5601\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5687 - d_loss: -73.4109 - g_loss: 72.9582 - gp: 3.0356\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5837 - d_loss: -71.8873 - g_loss: 82.1766 - gp: 3.5303\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.6079 - d_loss: -72.8463 - g_loss: 84.3929 - gp: 3.3911\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5806 - d_loss: -68.2122 - g_loss: 76.6118 - gp: 3.5044\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5922 - d_loss: -72.2774 - g_loss: 81.3372 - gp: 3.3607\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5929 - d_loss: -71.5842 - g_loss: 75.1922 - gp: 3.4147\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 1s 507ms/step - accuracy: 0.5831 - d_loss: -74.1861 - g_loss: 73.5115 - gp: 3.4860\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5940 - d_loss: -71.3806 - g_loss: 91.5346 - gp: 3.4991\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6186 - d_loss: -72.8958 - g_loss: 85.2781 - gp: 3.4564\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.6108 - d_loss: -72.1344 - g_loss: 92.4097 - gp: 3.4869\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.6052 - d_loss: -73.0948 - g_loss: 90.4718 - gp: 3.6740\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5925 - d_loss: -72.1103 - g_loss: 87.5004 - gp: 3.6340\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5881 - d_loss: -71.5512 - g_loss: 82.2532 - gp: 3.6428\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 1s 505ms/step - accuracy: 0.5896 - d_loss: -76.7610 - g_loss: 84.5283 - gp: 3.4746\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5659 - d_loss: -73.2365 - g_loss: 107.2606 - gp: 3.8671\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.6025 - d_loss: -70.3797 - g_loss: 87.4686 - gp: 3.7477\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5940 - d_loss: -76.4465 - g_loss: 85.3027 - gp: 3.2759\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5974 - d_loss: -74.5137 - g_loss: 91.9942 - gp: 3.4501\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5880 - d_loss: -72.7632 - g_loss: 89.4600 - gp: 3.8409\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6265 - d_loss: -74.0116 - g_loss: 99.4868 - gp: 3.5215\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6241 - d_loss: -73.7928 - g_loss: 88.1653 - gp: 3.8643\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5706 - d_loss: -80.9224 - g_loss: 99.4509 - gp: 3.5640\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.6062 - d_loss: -74.7364 - g_loss: 101.6504 - gp: 3.9622\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5711 - d_loss: -71.6856 - g_loss: 87.6526 - gp: 3.3537\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5976 - d_loss: -75.2950 - g_loss: 81.3449 - gp: 3.3281\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.6167 - d_loss: -76.9701 - g_loss: 90.5306 - gp: 3.5724\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.6152 - d_loss: -72.6320 - g_loss: 85.7964 - gp: 3.6204\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6263 - d_loss: -75.6288 - g_loss: 95.9490 - gp: 3.7356\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.6209 - d_loss: -72.7661 - g_loss: 93.3878 - gp: 3.6173\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.6155 - d_loss: -74.6586 - g_loss: 91.9566 - gp: 3.7172\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6125 - d_loss: -81.5233 - g_loss: 85.6441 - gp: 3.5799\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5942 - d_loss: -72.7822 - g_loss: 97.8291 - gp: 3.6447\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5970 - d_loss: -74.1746 - g_loss: 98.9470 - gp: 3.5397\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.6091 - d_loss: -73.5534 - g_loss: 88.3631 - gp: 3.3756\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6179 - d_loss: -79.0208 - g_loss: 122.5763 - gp: 3.6100\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 1s 504ms/step - accuracy: 0.6014 - d_loss: -68.2303 - g_loss: 92.8939 - gp: 3.4810\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.6051 - d_loss: -72.4725 - g_loss: 97.4891 - gp: 3.2480\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.6257 - d_loss: -79.9368 - g_loss: 92.1095 - gp: 3.6655\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6206 - d_loss: -70.7131 - g_loss: 96.6694 - gp: 3.9968\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6143 - d_loss: -70.7695 - g_loss: 107.4703 - gp: 3.7943\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6159 - d_loss: -74.1341 - g_loss: 103.9253 - gp: 3.5341\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5717 - d_loss: -75.5898 - g_loss: 97.9605 - gp: 3.4878\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5818 - d_loss: -79.6595 - g_loss: 110.6840 - gp: 3.6292\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.6101 - d_loss: -74.2351 - g_loss: 108.8482 - gp: 3.8560\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.6100 - d_loss: -75.8721 - g_loss: 99.4123 - gp: 3.5525\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5912 - d_loss: -74.8590 - g_loss: 109.4614 - gp: 3.7764\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5932 - d_loss: -69.5065 - g_loss: 107.9470 - gp: 3.6478\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5952 - d_loss: -76.3292 - g_loss: 100.2729 - gp: 3.2864\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6191 - d_loss: -67.4879 - g_loss: 107.9361 - gp: 4.0493\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.6196 - d_loss: -84.2324 - g_loss: 97.7826 - gp: 3.1654\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.6007 - d_loss: -76.7215 - g_loss: 93.8917 - gp: 3.7757\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5706 - d_loss: -82.0665 - g_loss: 100.6121 - gp: 3.5623\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5793 - d_loss: -77.1535 - g_loss: 99.3668 - gp: 3.6337\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6001 - d_loss: -73.9398 - g_loss: 101.5502 - gp: 3.7615\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 1s 503ms/step - accuracy: 0.6045 - d_loss: -79.0418 - g_loss: 92.8207 - gp: 3.3112\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6266 - d_loss: -77.3498 - g_loss: 126.6849 - gp: 4.1772\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6009 - d_loss: -80.6222 - g_loss: 90.4131 - gp: 3.3059\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 1s 508ms/step - accuracy: 0.6118 - d_loss: -74.7695 - g_loss: 91.5276 - gp: 3.8222\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.6065 - d_loss: -70.9112 - g_loss: 91.1782 - gp: 4.0313\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 499ms/step - accuracy: 0.5931 - d_loss: -77.7600 - g_loss: 104.2417 - gp: 3.6296\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.6029 - d_loss: -79.1932 - g_loss: 102.8280 - gp: 3.5611\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5995 - d_loss: -73.2933 - g_loss: 97.1937 - gp: 3.5731\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.6052 - d_loss: -75.5373 - g_loss: 97.0472 - gp: 3.5665\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5847 - d_loss: -74.4848 - g_loss: 99.4589 - gp: 3.9174\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5810 - d_loss: -78.0833 - g_loss: 105.9627 - gp: 3.6743\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5981 - d_loss: -75.6170 - g_loss: 105.6422 - gp: 3.2782\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6091 - d_loss: -70.5844 - g_loss: 100.5758 - gp: 3.6593\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5999 - d_loss: -72.0584 - g_loss: 97.4603 - gp: 3.5078\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.6047 - d_loss: -77.9927 - g_loss: 101.7764 - gp: 3.7320\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.6076 - d_loss: -74.4981 - g_loss: 105.4990 - gp: 3.9925\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5799 - d_loss: -73.4956 - g_loss: 99.1736 - gp: 3.6889\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5840 - d_loss: -77.6548 - g_loss: 86.0875 - gp: 3.4293\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5809 - d_loss: -75.3692 - g_loss: 96.3362 - gp: 3.8728\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5881 - d_loss: -71.9312 - g_loss: 100.4445 - gp: 3.8519\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5596 - d_loss: -75.0849 - g_loss: 89.1032 - gp: 3.6694\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 1s 504ms/step - accuracy: 0.5820 - d_loss: -74.7078 - g_loss: 95.8279 - gp: 3.3593\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 1s 504ms/step - accuracy: 0.5777 - d_loss: -75.2445 - g_loss: 93.9801 - gp: 3.4110\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 1s 519ms/step - accuracy: 0.5907 - d_loss: -71.9274 - g_loss: 96.4365 - gp: 3.6310\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 1s 524ms/step - accuracy: 0.5944 - d_loss: -75.2458 - g_loss: 99.5283 - gp: 3.7507\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5955 - d_loss: -76.8008 - g_loss: 98.2282 - gp: 3.6444\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5772 - d_loss: -78.2163 - g_loss: 99.4661 - gp: 3.6932\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5868 - d_loss: -75.7688 - g_loss: 85.9983 - gp: 3.6366\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5794 - d_loss: -72.0909 - g_loss: 107.2009 - gp: 3.8471\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5806 - d_loss: -73.0245 - g_loss: 98.6760 - gp: 3.3933\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 490ms/step - accuracy: 0.5857 - d_loss: -77.3721 - g_loss: 95.5622 - gp: 3.5941\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 497ms/step - accuracy: 0.5633 - d_loss: -74.3361 - g_loss: 96.2938 - gp: 3.6472\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 486ms/step - accuracy: 0.5620 - d_loss: -86.0826 - g_loss: 104.1003 - gp: 3.7080\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5916 - d_loss: -78.4389 - g_loss: 106.8723 - gp: 3.9992\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5697 - d_loss: -72.8066 - g_loss: 92.2381 - gp: 3.7209\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.6027 - d_loss: -71.5500 - g_loss: 97.5844 - gp: 3.7833\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5778 - d_loss: -69.3890 - g_loss: 98.6301 - gp: 3.6531\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 495ms/step - accuracy: 0.5988 - d_loss: -75.2023 - g_loss: 106.4245 - gp: 3.4002\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 487ms/step - accuracy: 0.5928 - d_loss: -70.8767 - g_loss: 104.2537 - gp: 3.4501\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5998 - d_loss: -66.9579 - g_loss: 99.0638 - gp: 3.6103\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5935 - d_loss: -75.6201 - g_loss: 90.2630 - gp: 3.3319\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5792 - d_loss: -74.9434 - g_loss: 100.9679 - gp: 3.5949\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5783 - d_loss: -77.8691 - g_loss: 106.7240 - gp: 3.4989\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 496ms/step - accuracy: 0.5848 - d_loss: -77.7855 - g_loss: 112.1758 - gp: 3.8572\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 1s 502ms/step - accuracy: 0.5660 - d_loss: -72.3057 - g_loss: 97.1582 - gp: 3.5833\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 1s 510ms/step - accuracy: 0.5710 - d_loss: -71.9881 - g_loss: 96.7397 - gp: 3.4382\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 1s 511ms/step - accuracy: 0.5720 - d_loss: -74.2506 - g_loss: 97.0874 - gp: 3.4151\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 1s 500ms/step - accuracy: 0.5684 - d_loss: -71.1680 - g_loss: 105.7332 - gp: 3.5255\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5751 - d_loss: -69.5697 - g_loss: 90.3765 - gp: 3.5733\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5682 - d_loss: -66.5075 - g_loss: 109.8792 - gp: 3.6506\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5723 - d_loss: -71.2905 - g_loss: 87.3591 - gp: 3.3496\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.5770 - d_loss: -74.4971 - g_loss: 101.8040 - gp: 3.5107\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5802 - d_loss: -71.9313 - g_loss: 101.4060 - gp: 3.5924\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 493ms/step - accuracy: 0.5661 - d_loss: -69.0585 - g_loss: 101.2223 - gp: 3.4163\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5437 - d_loss: -73.9419 - g_loss: 85.1305 - gp: 3.3052\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 484ms/step - accuracy: 0.5662 - d_loss: -71.0599 - g_loss: 99.7840 - gp: 3.6791\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5752 - d_loss: -68.0305 - g_loss: 92.8747 - gp: 3.3727\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 488ms/step - accuracy: 0.5806 - d_loss: -70.6583 - g_loss: 98.0516 - gp: 3.4492\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 479ms/step - accuracy: 0.5703 - d_loss: -78.3057 - g_loss: 92.6283 - gp: 3.5245\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5778 - d_loss: -73.4864 - g_loss: 85.2152 - gp: 3.7225\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5764 - d_loss: -75.2880 - g_loss: 107.4934 - gp: 3.3909\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5829 - d_loss: -73.6279 - g_loss: 104.2249 - gp: 3.3620\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 492ms/step - accuracy: 0.6067 - d_loss: -68.0333 - g_loss: 110.9827 - gp: 3.7622\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 480ms/step - accuracy: 0.5928 - d_loss: -74.3200 - g_loss: 96.5695 - gp: 3.1804\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5885 - d_loss: -74.2294 - g_loss: 95.1215 - gp: 3.4933\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5683 - d_loss: -70.0849 - g_loss: 109.8062 - gp: 3.8992\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 1s 523ms/step - accuracy: 0.5485 - d_loss: -70.3512 - g_loss: 107.4571 - gp: 3.5894\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5493 - d_loss: -70.4493 - g_loss: 101.4436 - gp: 3.2505\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 485ms/step - accuracy: 0.5995 - d_loss: -70.3967 - g_loss: 114.3711 - gp: 3.5912\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 494ms/step - accuracy: 0.5847 - d_loss: -70.3006 - g_loss: 103.0080 - gp: 3.3997\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5871 - d_loss: -68.9350 - g_loss: 91.7266 - gp: 3.3011\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 489ms/step - accuracy: 0.5798 - d_loss: -74.0122 - g_loss: 108.0870 - gp: 3.4838\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 478ms/step - accuracy: 0.5845 - d_loss: -67.9017 - g_loss: 99.3296 - gp: 3.4845\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5879 - d_loss: -70.1009 - g_loss: 104.6219 - gp: 3.4682\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 477ms/step - accuracy: 0.5637 - d_loss: -72.2951 - g_loss: 114.9967 - gp: 3.7158\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 498ms/step - accuracy: 0.5690 - d_loss: -75.3637 - g_loss: 101.1326 - gp: 3.2751\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 483ms/step - accuracy: 0.5780 - d_loss: -74.0616 - g_loss: 88.3739 - gp: 3.4812\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5704 - d_loss: -71.0603 - g_loss: 101.1691 - gp: 3.5689\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 482ms/step - accuracy: 0.5659 - d_loss: -71.9288 - g_loss: 101.8343 - gp: 3.4729\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 481ms/step - accuracy: 0.5638 - d_loss: -74.0522 - g_loss: 106.3201 - gp: 3.3392\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 491ms/step - accuracy: 0.5702 - d_loss: -68.4290 - g_loss: 105.6168 - gp: 3.5856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqiMswXWWMd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}